{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4452c7-cd46-48fe-a601-fdba2ff17d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "import re  # Import the regular expression module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from scipy.stats import norm \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6ce943-38c4-40dd-908f-fd2b5273a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_dir = \"C:/Users/anaho/Desktop/research/Ayahuasca/DATA/FinalCodings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb786c-2a00-457d-a523-dabbd6084a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Get ayahuasca users data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e7904b-e2d0-43eb-9c9c-d4c0f3b1149c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Get all DOCX files in the directory, sorted alphanumerically\n",
    "docx_files = sorted([f for f in os.listdir(docx_dir) if f.endswith(\".docx\")])\n",
    "\n",
    "all_data = {}  # Store data from all files, keyed by participant ID\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#make question dict\n",
    "question_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71bf379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 tables in New_Coding_01.docx\n",
      "  Processing table 1 of 3 in New_Coding_01.docx\n",
      "  Processing table 2 of 3 in New_Coding_01.docx\n",
      "  Processing table 3 of 3 in New_Coding_01.docx\n",
      "Processing 3 tables in New_Coding_02.docx\n",
      "  Processing table 1 of 3 in New_Coding_02.docx\n",
      "  Processing table 2 of 3 in New_Coding_02.docx\n",
      "  Processing table 3 of 3 in New_Coding_02.docx\n",
      "Processing 3 tables in New_Coding_04.docx\n",
      "  Processing table 1 of 3 in New_Coding_04.docx\n",
      "  Processing table 2 of 3 in New_Coding_04.docx\n",
      "  Processing table 3 of 3 in New_Coding_04.docx\n",
      "Processing 3 tables in New_Coding_05.docx\n",
      "  Processing table 1 of 3 in New_Coding_05.docx\n",
      "  Processing table 2 of 3 in New_Coding_05.docx\n",
      "  Processing table 3 of 3 in New_Coding_05.docx\n",
      "Processing 3 tables in New_Coding_06.docx\n",
      "  Processing table 1 of 3 in New_Coding_06.docx\n",
      "  Processing table 2 of 3 in New_Coding_06.docx\n",
      "  Processing table 3 of 3 in New_Coding_06.docx\n",
      "Processing 3 tables in New_Coding_07.docx\n",
      "  Processing table 1 of 3 in New_Coding_07.docx\n",
      "  Processing table 2 of 3 in New_Coding_07.docx\n",
      "  Processing table 3 of 3 in New_Coding_07.docx\n",
      "Processing 3 tables in New_Coding_08.docx\n",
      "  Processing table 1 of 3 in New_Coding_08.docx\n",
      "  Processing table 2 of 3 in New_Coding_08.docx\n",
      "  Processing table 3 of 3 in New_Coding_08.docx\n",
      "Processing 3 tables in New_Coding_09.docx\n",
      "  Processing table 1 of 3 in New_Coding_09.docx\n",
      "  Processing table 2 of 3 in New_Coding_09.docx\n",
      "  Processing table 3 of 3 in New_Coding_09.docx\n",
      "Processing 3 tables in New_Coding_10.docx\n",
      "  Processing table 1 of 3 in New_Coding_10.docx\n",
      "  Processing table 2 of 3 in New_Coding_10.docx\n",
      "  Processing table 3 of 3 in New_Coding_10.docx\n",
      "Processing 3 tables in New_Coding_11.docx\n",
      "  Processing table 1 of 3 in New_Coding_11.docx\n",
      "  Processing table 2 of 3 in New_Coding_11.docx\n",
      "  Processing table 3 of 3 in New_Coding_11.docx\n",
      "Processing 3 tables in New_Coding_12.docx\n",
      "  Processing table 1 of 3 in New_Coding_12.docx\n",
      "  Processing table 2 of 3 in New_Coding_12.docx\n",
      "  Processing table 3 of 3 in New_Coding_12.docx\n",
      "Processing 3 tables in New_Coding_13.docx\n",
      "  Processing table 1 of 3 in New_Coding_13.docx\n",
      "  Processing table 2 of 3 in New_Coding_13.docx\n",
      "  Processing table 3 of 3 in New_Coding_13.docx\n",
      "Processing 3 tables in New_Coding_14.docx\n",
      "  Processing table 1 of 3 in New_Coding_14.docx\n",
      "  Processing table 2 of 3 in New_Coding_14.docx\n",
      "  Processing table 3 of 3 in New_Coding_14.docx\n",
      "Processing 3 tables in New_Coding_15.docx\n",
      "  Processing table 1 of 3 in New_Coding_15.docx\n",
      "  Processing table 2 of 3 in New_Coding_15.docx\n",
      "  Processing table 3 of 3 in New_Coding_15.docx\n"
     ]
    }
   ],
   "source": [
    " # 2. Iterate through each DOCX file\n",
    "for docx_file in docx_files:\n",
    "    docx_path = os.path.join(docx_dir, docx_file)\n",
    "    try:\n",
    "        document = docx.Document(docx_path)\n",
    "        num_tables = len(document.tables)\n",
    "        print(f\"Processing {num_tables} tables in {docx_file}\")\n",
    "        if num_tables == 0:\n",
    "            print(f\"No tables found in {docx_file}, skipping.\")\n",
    "            continue\n",
    "\n",
    "    except docx.exceptions.EmptyPackageError as e:\n",
    "        print(f\"Error reading {docx_file}: {e}.  Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    # Extract participant name from filename (remove \".docx\")\n",
    "    participant_name = os.path.splitext(docx_file)[0]  # e.g., \"codage_01\"\n",
    "\n",
    "    # Extract participant ID (e.g., \"01\")\n",
    "    participant_id = participant_name[-2:]\n",
    "    \n",
    "    all_table_data= [] #instantiate list\n",
    "\n",
    "    # 3. Iterate through each table in the document\n",
    "    for table_index, table in enumerate(document.tables):\n",
    "        print(f\"  Processing table {table_index + 1} of {num_tables} in {docx_file}\")\n",
    "        table_data = {}  # Store data for the current table, keyed by question code\n",
    "        # 4. Extract data from the table, handling LetterNumber rows\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]  # Remove extra whitespace\n",
    "            if cells:\n",
    "                first_cell_value = cells[0]\n",
    "                if re.match(r\"^[A-Za-z]\\d+$\", first_cell_value) or re.match(r\"^[A-Za-z]\\d[A-Za-z]+$\", first_cell_value) or re.match(r\"^[A-Za-z]\\d\\d[A-Za-z]+$\", first_cell_value):\n",
    "                    question_code = first_cell_value\n",
    "                    question_text = cells[1]  # Extract the question text\n",
    "                    yes_val = cells[2].lower()\n",
    "                    no_val = cells[3].lower()\n",
    "                    note = cells[4]\n",
    "                    response = None # Initialize response\n",
    "\n",
    "                    if \"x\" in yes_val:\n",
    "                        response = 1.0\n",
    "                    elif \"x\" in no_val:\n",
    "                        response = 0.0\n",
    "                    else:\n",
    "                         response = 0.0 #replacing by 0 the missing x\n",
    "\n",
    "                    table_data[question_code] = {\"response\": response, \"note\": note}\n",
    "                    question_dict[question_code] = question_text #store the question and text.\n",
    "\n",
    "        \n",
    "            '''    else:\n",
    "                    print(\n",
    "                        f\"    Skipping row in {docx_file}, table {table_index + 1} because the first cell '{first_cell_value}' does not match 'A1' format.\")\n",
    "            else:\n",
    "                print(f\"    Skipping empty row in {docx_file}, table {table_index + 1}.\")'''\n",
    "\n",
    "        # 5. Store data for this participant and question\n",
    "        all_table_data.append(table_data)  # Use participant_id\n",
    "\n",
    "    # Flatten the list of dictionaries into a single dictionary for the participant\n",
    "    all_table_data_flat = {}\n",
    "    for item in all_table_data:\n",
    "        all_table_data_flat.update(item)\n",
    "\n",
    "    all_data[participant_id] = all_table_data_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3883de19-12de-4f01-81d7-43c76d843e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # 6. Prepare data for Excel, transposing and combining\n",
    "# Prepare data for Excel sheet\n",
    "excel_data = []\n",
    "# Get all unique questions\n",
    "unique_questions = set()\n",
    "for data in all_data.values():\n",
    "    unique_questions.update(data.keys())\n",
    "unique_questions = sorted(list(unique_questions))\n",
    "\n",
    "# Create the header row\n",
    "header_row = [\"Participants\"]  # Changed Header\n",
    "for question in unique_questions:\n",
    "    header_row.extend([f\"{question}\"])\n",
    "excel_data.append(header_row)\n",
    "\n",
    "# Create participant rows\n",
    "participant_ids = sorted(list(all_data.keys()))  # Get unique participant IDs\n",
    "for participant_id in participant_ids:\n",
    "    participant_data = all_data[participant_id]\n",
    "    participant_row = [participant_id]  # Add participant ID here\n",
    "    for question in unique_questions:\n",
    "        if question in participant_data:\n",
    "            #participant_row.extend([participant_data[question]['response'], participant_data[question]['note']]) #including notes\n",
    "            participant_row.extend([participant_data[question]['response']])  #only codes, no notes\n",
    "        if question not in participant_data:\n",
    "            print(question)\n",
    "    excel_data.append(participant_row)\n",
    "\n",
    "# 7. Put everything in a dataframe!!\n",
    "df = pd.DataFrame(excel_data, columns = header_row).drop([0])\n",
    "df_final = df.transpose().drop(['Participants'])\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "for col in df_final.columns:\n",
    "    df_final[col] = pd.to_numeric(df_final[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87913031-6289-4131-9a8c-3f696dace795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created row 'T1' by comparing 'H1' and 'P1'.\n",
      "Created row 'T2' by comparing 'H2' and 'P2'.\n",
      "Created row 'T3' by comparing 'H3' and 'P3'.\n",
      "Created row 'T4' by comparing 'H4' and 'P4'.\n",
      "Created row 'T5' by comparing 'H5' and 'P5'.\n",
      "Created row 'T6' by comparing 'H6' and 'P6'.\n",
      "Created row 'T7' by comparing 'H7' and 'P7'.\n",
      "Created row 'T8' by comparing 'H8' and 'P8'.\n",
      "Created row 'T9' by comparing 'Q1' and 'I1'.\n",
      "Created row 'T10' by comparing 'Q2' and 'I2'.\n",
      "Created row 'T11' by comparing 'Q3' and 'I3'.\n",
      "Created row 'T12' by comparing 'Q4' and 'I4a'.\n",
      "Created row 'T13' by comparing 'Q5' and 'I5'.\n",
      "Created row 'T14' by comparing 'Q6' and 'I6'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1    2    3    4    5    6    7    8    9    10   11   12   13   14\n",
       "A1   0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "A2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "A3   0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "A4   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "A5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "T10  0.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "T11  0.0  0.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "T12  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       "T13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "T14  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[204 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep biggest value for outcome and interpreration\n",
    "homo_codes = {'H1':'P1','H2':'P2','H3':'P3','H4':'P4','H5':'P5','H6':'P6','H7':'P7','H8':'P8',\n",
    "              'Q1':'I1','Q2':'I2','Q3':'I3', 'Q4':'I4a','Q5':'I5','Q6':'I6'} #homologous codes\n",
    "\n",
    "\n",
    "counter = 1 # Initialize a counter for your T1, T2, T3... row names\n",
    "\n",
    "for h_code, p_i_code in homo_codes.items():\n",
    "    # Check if both rows exist in the DataFrame to prevent errors\n",
    "    if h_code in df_final.index and p_i_code in df_final.index:\n",
    "        # Generate the new row name (e.g., 'T1', 'T2')\n",
    "        new_row_name = f'T{counter}'\n",
    "        question_dict[f'T{counter}'] = 'overall ' + question_dict[h_code]\n",
    "\n",
    "        # Perform the comparison and store the biggest value in the new row\n",
    "        df_final.loc[new_row_name] = np.maximum(df_final.loc[h_code], df_final.loc[p_i_code])\n",
    "\n",
    "        print(f\"Created row '{new_row_name}' by comparing '{h_code}' and '{p_i_code}'.\")\n",
    "        counter += 1 # Increment the counter for the next row\n",
    "    else:\n",
    "        print(f\"Warning: Skipping comparison for '{h_code}' and '{p_i_code}'. One or both indices not found in DataFrame.\")\n",
    "\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559b5f2a-8f52-48e3-8896-e647a45708e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Total'] = df_final.astype(float).sum(axis = 1)\n",
    "\n",
    "#add col containing textual question\n",
    "df_final['Question_text'] = df_final.index.map(question_dict)\n",
    "\n",
    "#get percentages\n",
    "df_final['Percentages'] = np.round(df_final['Total']/14*100,2) #dep on the nb of part\n",
    "\n",
    "df_final.loc['J1':'O9','Percentages'] = np.round(df_final.loc['J1':'O9','Total']/14*100,2) #only 14 reported a form of VH after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454aa1e9-3d23-424b-8f04-6cb7a542256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(docx_dir + 'descriptive_stats.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13507a2d-4561-41a5-a5cf-01695e9d6337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First experiences connected to use of psychedelics or other drugs?',\n",
       " 'Volitional',\n",
       " 'Non-volitional',\n",
       " 'Ability to influence voice',\n",
       " 'Change in influence over time',\n",
       " 'Change in frequency over time',\n",
       " 'Voice character change over time',\n",
       " 'Olfactory hallucination',\n",
       " 'Gustatory hallucination',\n",
       " 'Dissociative experiences',\n",
       " 'Non-verbal sounds',\n",
       " 'Visual hallucination',\n",
       " 'Tactile hallucination',\n",
       " '‘Boundary’ voices',\n",
       " '‘Egocentric voices',\n",
       " '‘Allocentric’ voices',\n",
       " 'Depression-associated',\n",
       " 'Paranoia-associated',\n",
       " 'Absent agency',\n",
       " 'Agency without individuation',\n",
       " 'Internally individualised agency',\n",
       " 'Calm, relaxed state',\n",
       " 'Anxious, stressed or alert state',\n",
       " 'Depressed state',\n",
       " 'Events that recently happened',\n",
       " 'Absence of surrounding people',\n",
       " 'Frequency',\n",
       " 'Intensity',\n",
       " 'Decreasing sense of control over time',\n",
       " 'Negative impact on relationships',\n",
       " 'Family narrative',\n",
       " 'Self-stigma regarding voices',\n",
       " '‘Auditory’ voices',\n",
       " 'Olfactory hallucination',\n",
       " 'Gustatory hallucination',\n",
       " 'Dissociative experiences',\n",
       " 'Non-verbal sounds',\n",
       " 'Tactile hallucination',\n",
       " '‘Boundary’ voices',\n",
       " '‘Egocentric voices',\n",
       " '‘Allocentric’ voices',\n",
       " 'Voice character change over time',\n",
       " 'Paranoia-associated',\n",
       " 'Absent agency',\n",
       " 'Agency without individuation',\n",
       " 'Internally individualised agency',\n",
       " 'Tactile contacts',\n",
       " 'Depressed state',\n",
       " 'Events that recently happened',\n",
       " 'Presence or actions of facilitators',\n",
       " 'Music',\n",
       " 'Does the first voice hearing experience was volitional?',\n",
       " 'Decreasing sense of control over time',\n",
       " 'Negative impact on relationships',\n",
       " 'Family narrative',\n",
       " 'Self-stigma regarding voices',\n",
       " 'Sense of unity',\n",
       " 'Transcendence of space and time',\n",
       " 'Ineffability',\n",
       " 'Positive mood',\n",
       " 'Transcendence of space and time',\n",
       " 'overall Negative impact on relationships',\n",
       " 'overall Family narrative',\n",
       " 'overall Self-stigma regarding voices']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#very low scores\n",
    "list(df_final[df_final['Total']<2].Question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1504f6c1-3a34-4bd3-a28a-4350bade86b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>Total</th>\n",
       "      <th>Question_text</th>\n",
       "      <th>Percentages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I4b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Spiritual beliefs preceded first VH experience</td>\n",
       "      <td>57.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3    4    5    6    7    8    9   10   11   12   13   14  \\\n",
       "I4b  1.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0   \n",
       "\n",
       "     Total                                   Question_text  Percentages  \n",
       "I4b    8.0  Spiritual beliefs preceded first VH experience        57.14  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['Question_text'] == 'Spiritual beliefs preceded first VH experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00171660-8014-4020-8df3-466018c7ded5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bodily states',\n",
       " '‘Thought-like’ voices',\n",
       " 'Multisensory voices',\n",
       " 'Visual imagery',\n",
       " 'Internally located voices?',\n",
       " 'Recurring voices',\n",
       " 'Direct address',\n",
       " 'Commenting voices?',\n",
       " 'Conversational voices?',\n",
       " 'Commanding voices?',\n",
       " 'Follow commands?',\n",
       " 'Positive/helpful voices?',\n",
       " 'Contains a message',\n",
       " 'Recognisable voices',\n",
       " 'Voice knowledge',\n",
       " 'Complex personification',\n",
       " 'Archetypal features',\n",
       " 'Voices elicit positive emotions',\n",
       " 'Voices elicit negative emotions',\n",
       " 'Externally individualised agency',\n",
       " 'Presence of voices',\n",
       " 'State of mind',\n",
       " 'Presence or actions of facilitators',\n",
       " 'Music',\n",
       " 'Impact of voices on quality of life',\n",
       " 'Pedagogical outcome',\n",
       " 'Therapeutic outcome',\n",
       " 'Spiritual/religious outcome',\n",
       " 'Supernatural/spiritual narrative',\n",
       " 'Spiritual beliefs preceded first VH experience',\n",
       " 'Bodily states',\n",
       " '‘Thought-like’ voices',\n",
       " 'Multisensory voices',\n",
       " 'Visual imagery',\n",
       " 'Felt presences',\n",
       " 'Internally located voices?',\n",
       " 'Recurring voices',\n",
       " 'Direct address',\n",
       " 'Commenting voices?',\n",
       " 'Commanding voices?',\n",
       " 'Follow commands?',\n",
       " 'Positive/helpful voices?',\n",
       " 'Contains a message',\n",
       " 'Recognisable voices',\n",
       " 'Voice knowledge',\n",
       " 'Complex personification',\n",
       " 'Archetypal features',\n",
       " 'Voices elicit positive emotions',\n",
       " 'Voices elicit negative emotions',\n",
       " 'Externally individualised agency',\n",
       " 'Presence of voices',\n",
       " 'State of mind',\n",
       " 'Presence/absence of the voice',\n",
       " 'Emotional tone',\n",
       " 'Development of cultivation of voices technics over time',\n",
       " 'Impact of voices on quality of life',\n",
       " 'Pedagogical outcome',\n",
       " 'Therapeutic outcome',\n",
       " 'Spiritual/religious outcome',\n",
       " 'Impact on metaphysical views',\n",
       " 'Supernatural/spiritual narrative',\n",
       " 'overall Impact of voices on quality of life',\n",
       " 'overall Pedagogical outcome',\n",
       " 'overall Therapeutic outcome',\n",
       " 'overall Spiritual/religious outcome',\n",
       " 'overall Impact on metaphysical views',\n",
       " 'overall Supernatural/spiritual narrative']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#more than majority\n",
    "list(df_final[df_final['Total']>=8].Question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "758a1d0b-b1a3-4dfa-9502-21167c3f009c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘Thought-like’ voices',\n",
       " 'Internally located voices?',\n",
       " 'Recurring voices',\n",
       " 'Positive/helpful voices?',\n",
       " 'Contains a message',\n",
       " 'Voice knowledge',\n",
       " 'Voices elicit positive emotions',\n",
       " 'Externally individualised agency',\n",
       " '‘Thought-like’ voices',\n",
       " 'Positive/helpful voices?',\n",
       " 'Contains a message',\n",
       " 'Recognisable voices',\n",
       " 'Voices elicit positive emotions',\n",
       " 'Impact of voices on quality of life',\n",
       " 'Supernatural/spiritual narrative',\n",
       " 'overall Impact of voices on quality of life',\n",
       " 'overall Supernatural/spiritual narrative']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#very high scores\n",
    "list(df_final[df_final['Total']>=13].Question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8040a3b-0aae-4df7-86df-82b4243f55e5",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Comparing with other groups from Moseley et al. 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab40589-39d6-4ec1-bffb-511513fec123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data from Mosely et al., 2023  paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5836a01-d968-419e-bb4b-3c24e3746e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table1 = np.array([['Visual imagery', 100, 5.0],\n",
    "                 ['Auditory', 84.6, 92.5],\n",
    "                 ['Internally located', 84.6, 62.5],\n",
    "                 ['Egocentric', 80.8, 70.0],\n",
    "                 ['Thought-like', 76.9, 52.5],\n",
    "                 ['Externally located', 73.1, 80.0],\n",
    "                 ['Felt presence', 69.2, 52.5],\n",
    "                 ['Multimodal', 69.2, 27.5],\n",
    "                 ['Visual', 60.0, 75.0],\n",
    "                 ['Bodily states', 53.8, 65.0],\n",
    "                 ['Olfactory', 50.0, 37.5],\n",
    "                 ['Dissociative', 46.2, 30.0],\n",
    "                 ['Tactile', 46.2, 22.5],\n",
    "                 ['Gustatory', 26.9, 0],\n",
    "                 ['Nonverbal', 26.9, 40.0],\n",
    "                 ['Allocentric', 23.1, 37.5],\n",
    "                 ['Boundary', 11.5, 35.0],\n",
    "                 ['Elicit positive emotions', 100 ,35]])\n",
    "                 \n",
    "data_table2= np.array([['Voice knowledge', 96.2, 45],\n",
    "                 ['Volitional', 92.3, 2.5],\n",
    "                 ['Nonvolitional', 92.3, 100],\n",
    "                 ['Positive/helpful', 92.3, 42.5],\n",
    "                 ['Recurring', 92.3, 92.5],\n",
    "                 ['Ability to influence', 84.6, 27.5],\n",
    "                 ['Change in frequency', 80.8, 70],\n",
    "                 ['Recognizable', 76.9, 47.5],\n",
    "                 ['Change in influence', 73.1, 12.5],\n",
    "                 ['Simple structure', 69.2, 40 ],    \n",
    "                 ['Conversational', 65.4, 47.5 ],     \n",
    "                 ['Direct address', 61.5, 82.5 ],     \n",
    "                 ['Structural change', 50, 80],     \n",
    "                 ['First voice traumatic event', 42.3, 65],   \n",
    "                 ['Companionship', 42.3, 32.5],     \n",
    "                 ['Elicit negative emotions', 38.5, 100 ],  \n",
    "                 ['Commanding', 26.9, 67.5],\n",
    "                 ['Commenting', 19.2, 45  ],   \n",
    "                 ['Character change', 11.5, 17.5],    \n",
    "                 ['Abusive/violent', 11.5, 87.5]])    \n",
    "                      \n",
    "data_table3 =np.array([['Supernatural narrative', 100, 20],\n",
    "                  ['Internally individualized', 100, 75],    \n",
    "                  ['Externally individualized', 80.8, 50],      \n",
    "                  ['Minimal personification', 61.5, 60],      \n",
    "                  ['Important to identity', 57.7, 12.5],      \n",
    "                  ['Complex personification', 38.5, 40],     \n",
    "                  ['Agency w/o individuation', 38.5, 45],\n",
    "                  ['Self-stigma', 30.8, 47.5],      \n",
    "                  ['Negative on relationships', 26.9, 77.5],      \n",
    "                  ['Biophysical narrative', 23.1, 25  ],     \n",
    "                  ['Archetypal', 23.1, 42.5],     \n",
    "                  ['Family narrative', 19.2, 22.5],      \n",
    "                  ['Positive on relationships', 15.4, 5],   \n",
    "                  ['Trauma narrative', 15.4, 25],    \n",
    "                  ['Sleep disruption', 15.4, 62.5],      \n",
    "                  ['Idiosyncratic narrative', 11.5, 32.5],      \n",
    "                  ['Stress narrative', 11.5, 37.5],      \n",
    "                  ['Absent agency', 11.5, 15],    \n",
    "                  ['Suicidality', 3.8, 50]])      \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90e40ad-4bb5-469e-8da1-702e7257c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionnaries to find codes reported in Moseley paper in our data\n",
    "#during ayahuasca\n",
    "dict_during = {  'Visual imagery': 'B7',\n",
    "                 'Auditory': 'B1',\n",
    "                 'Internally located': 'C1',\n",
    "                 'Egocentric': 'C4',\n",
    "                 'Thought-like': 'B2',\n",
    "                 'Externally located': 'C2',\n",
    "                 'Felt presence': 'B8',\n",
    "                 'Multimodal': 'B4',\n",
    "                 'Visual': 'B6',\n",
    "                 'Bodily states': 'B13',\n",
    "                 'Olfactory': 'B10',\n",
    "                 'Dissociative': 'B12',\n",
    "                 'Tactile': 'B9',\n",
    "                 'Gustatory': 'B11',\n",
    "                 'Nonverbal': 'B5',\n",
    "                 'Allocentric': 'C5',\n",
    "                 'Boundary': 'C3',\n",
    "                 'Elicit positive emotions': 'D7',\n",
    "                 'Voice knowledge': 'D20',\n",
    "                 'Volitional': 'G2',\n",
    "                 'Nonvolitional': None ,\n",
    "                 'Positive/helpful': 'D17',\n",
    "                 'Recurring' : 'D1',\n",
    "                 'Ability to influence': 'X1', #if any control (g) is 1\n",
    "                 'Change in frequency': 'D21',\n",
    "                 'Recognizable': 'D2',\n",
    "                 'Change in influence' : 'G9', #unsure\n",
    "                 'Simple structure': 'D10',\n",
    "                 'Conversational': 'D13',\n",
    "                 'Direct address': 'D11',\n",
    "                 'Structural change': None,\n",
    "                 'First voice traumatic event' : None,\n",
    "                 'Companionship': 'D18',\n",
    "                 'Elicit negative emotions': 'D8',\n",
    "                 'Commanding': 'D14',\n",
    "                 'Commenting': 'D12',\n",
    "                 'Character change': 'D6',\n",
    "                 'Abusive/violent': 'D16',\n",
    "                 'Absent agency': 'E1',\n",
    "                 'Internally individualized': 'E3',\n",
    "                 'Externally individualized': 'E4',\n",
    "                 'Minimal personification': 'D3',\n",
    "                 'Important to identity': None,\n",
    "                 'Complex personification': 'D4',\n",
    "                 'Archetypal' : 'D5',\n",
    "                 'Agency w/o individuation': 'E2'}\n",
    "\n",
    "dict_after = {  'Visual imagery': 'J7',\n",
    "                 'Auditory': 'J1',\n",
    "                 'Internally located': 'K1',\n",
    "                 'Egocentric': 'K4',\n",
    "                 'Thought-like': 'J2',\n",
    "                 'Externally located': 'K2',\n",
    "                 'Felt presence': 'J8',\n",
    "                 'Multimodal': 'J4',\n",
    "                 'Visual': 'J6',\n",
    "                 'Bodily states': 'J13',\n",
    "                 'Olfactory': 'J10',\n",
    "                 'Dissociative': 'J12',\n",
    "                 'Tactile': 'J9',\n",
    "                 'Gustatory': 'J11',\n",
    "                 'Nonverbal': 'J5',\n",
    "                 'Allocentric': 'K5',\n",
    "                 'Boundary': 'K3',\n",
    "                 'Elicit positive emotions': 'L7',\n",
    "                 'Voice knowledge': 'L20',\n",
    "                 'Volitional': 'O2',\n",
    "                 'Nonvolitional': None ,\n",
    "                 'Positive/helpful': 'L17',\n",
    "                 'Recurring' : 'L1',\n",
    "                 'Ability to influence': 'X2', #if any control (O) is 1\n",
    "                 'Change in frequency': 'L21',\n",
    "                 'Recognizable': 'L2',\n",
    "                 'Change in influence' : 'O9', #unsure\n",
    "                 'Simple structure': 'L10',\n",
    "                 'Conversational': 'L13',\n",
    "                 'Direct address': 'L11',\n",
    "                 'Structural change': None,\n",
    "                 'First voice traumatic event' : None,\n",
    "                 'Companionship': 'L18',\n",
    "                 'Elicit negative emotions': 'L8',\n",
    "                 'Commanding': 'L14',\n",
    "                 'Commenting': 'L12',\n",
    "                 'Character change': 'L6',\n",
    "                 'Abusive/violent': 'L16',\n",
    "                 'Absent agency': 'M1',\n",
    "                 'Internally individualized': 'M3',\n",
    "                 'Externally individualized': 'M4',\n",
    "                 'Minimal personification': 'L3',\n",
    "                 'Important to identity': None,\n",
    "                 'Complex personification': 'L4',\n",
    "                 'Archetypal' : 'L5',\n",
    "                 'Agency w/o individuation': 'M2'}\n",
    "\n",
    "dict_both = {'Supernatural narrative': 'T12',\n",
    "             'Self-stigma': 'T14',\n",
    "             'Negative on relationships' : 'T3' ,\n",
    "             'Biophysical narrative': 'T9',\n",
    "             'Family narrative': 'T13',\n",
    "             'Positive on relationships': 'T2',\n",
    "             'Trauma narrative': None,\n",
    "             'Sleep disruption': None,\n",
    "             'Idiosyncratic narrative': 'T11',\n",
    "             'Stress narrative': None,\n",
    "             'Suicidality': None}\n",
    "\n",
    "#number of participants for confidence intervals calculations\n",
    "N_VIP = 40\n",
    "N_SPIRI = 26\n",
    "N_AYA = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6da78032-ec62-4745-a670-9cc8d920348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=14\n",
    "#during\n",
    "df_final.loc['X1',:N]  = df_final.loc[['G1','G2','G3','G4', 'G5', 'G6', 'G7', 'G8'], :N].sum(axis=0) > 0\n",
    "df_final.loc['X1', :N] = df_final.loc['X1', :N].astype(int)\n",
    "df_final.loc['X1','Question_text'] = 'Ability_to_control_during'\n",
    "df_final.loc['X1', 'Total'] = df_final.loc['X1', :N].sum()\n",
    "df_final.loc['X1', 'Percentages'] = df_final.loc['X1','Total']/N*100\n",
    "\n",
    "#sam for after\n",
    "df_final.loc['X2',:N]  = df_final.loc[['O1','O2','O3','O4', 'O5', 'O6', 'O7', 'O8'], :N].sum(axis=0) > 0\n",
    "df_final.loc['X2', :N] = df_final.loc['X2', :N].astype(int)\n",
    "df_final.loc['X2','Question_text'] = 'Ability_to_control_during'\n",
    "df_final.loc['X2', 'Total'] = df_final.loc['X2', :N].sum()\n",
    "df_final.loc['X2', 'Percentages'] = df_final.loc['X2','Total']/N*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81d4f4e-0b26-4d07-b987-f9950c5db059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>Total</th>\n",
       "      <th>Question_text</th>\n",
       "      <th>Percentages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I4b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Spiritual beliefs preceded first VH experience</td>\n",
       "      <td>57.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3    4    5    6    7    8    9   10   11   12   13   14  \\\n",
       "I4b  1.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0   \n",
       "\n",
       "     Total                                   Question_text  Percentages  \n",
       "I4b    8.0  Spiritual beliefs preceded first VH experience        57.14  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['Question_text'] == 'Spiritual beliefs preceded first VH experience']\n",
    "#df_final.loc[['X1','X2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c1002-d9d8-424b-b85a-5b24022d236d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Adding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c03f304d-ee61-4307-abae-403ac9a7b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the dataframe\n",
    "data_tables = np.vstack([data_table1, data_table2, data_table3])\n",
    "df_others = pd.DataFrame(data_tables, columns=['Code', 'Spiritual%', 'Psychosis%'])\n",
    "\n",
    "# Set the first element of each row as the index\n",
    "df_others.index = df_others['Code']\n",
    "df_others = df_others.drop('Code', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c1a795a-a7ee-4b4c-881b-4f22094bde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#force it to float\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "for col in df_others.columns:\n",
    "    df_others[col] = pd.to_numeric(df_others[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96c77c27-ea1a-4666-8a06-12a058d3dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the comparative DF with our percentages\n",
    "#category: both\n",
    "df_both = df_others\n",
    "for other_code, aya_code in dict_both.items():\n",
    "    if aya_code is not None:\n",
    "        df_both.loc[other_code,'Ayahuasca%'] = (df_final.loc[aya_code, 'Percentages'])\n",
    "df_both = df_both.dropna()\n",
    "        \n",
    "#category: during\n",
    "df_during = df_others\n",
    "for other_code, aya_code in dict_during.items():\n",
    "    if aya_code is not None:\n",
    "        df_during.loc[other_code,'Ayahuasca%'] = (df_final.loc[aya_code, 'Percentages'])\n",
    "df_during = df_during.dropna()\n",
    "        \n",
    "#category: after\n",
    "df_after = df_others\n",
    "for other_code, aya_code in dict_after.items():\n",
    "    if aya_code is not None:\n",
    "        df_after.loc[other_code,'Ayahuasca%'] = (df_final.loc[aya_code, 'Percentages'])\n",
    "df_after = df_after.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80685ed7-68e1-4c83-9347-d93315d23a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96124bfc-def1-4209-91a9-e1c1795c02ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate LOR, its 95% CI, and P-value\n",
    "def calculate_lor_and_ci(row, col_1, col_2, N_SPIRI = N_SPIRI, N_VIP= N_VIP): # Added N_SPIRI and N_VIP as arguments for clarity\n",
    "    # Define the output column names for returning NaNs\n",
    "    output_cols = ['LOR', 'LOR_CI_Lower', 'LOR_CI_Upper', 'P_Value'] # Added P_Value\n",
    "\n",
    "    p1 = row[col_1] / 100\n",
    "    p2 = row[col_2] / 100\n",
    "    n1 = N_SPIRI\n",
    "    n2 = N_VIP\n",
    "\n",
    "    #account for missing ayahuasca data\n",
    "    if any(pd.isna(row[col]) for col in [col_1,col_2]):\n",
    "        print(f\"Skipping row due to NaN in required columns: {row.name}\") # Optional: for debugging\n",
    "        return pd.Series([np.nan] * len(output_cols), index=output_cols)\n",
    "\n",
    "    # Convert proportions to counts (for the 2x2 table)\n",
    "    # Using 'success' for the percentage value, 'failure' for the rest\n",
    "    a = round(p1 * n1)\n",
    "    b = n1 - a\n",
    "    c = round(p2 * n2)\n",
    "    d = n2 - c\n",
    "\n",
    "    # Apply Haldane's correction for zero counts\n",
    "    # This is crucial for valid SE calculation and avoids inf/-inf\n",
    "    a_corr = a + 0.5\n",
    "    b_corr = b + 0.5\n",
    "    c_corr = c + 0.5\n",
    "    d_corr = d + 0.5\n",
    "\n",
    "    # Calculate Log Odds Ratio (using natural log)\n",
    "    try:\n",
    "        lor = math.log((a_corr * d_corr) / (b_corr * c_corr))\n",
    "    except ValueError: # This handles cases where any value is <= 0 after correction (shouldn't happen with 0.5)\n",
    "        lor = np.nan\n",
    "        se_lor = np.nan\n",
    "        lor_ci_lower = np.nan\n",
    "        lor_ci_upper = np.nan\n",
    "        or_ci_lower = np.nan\n",
    "        or_ci_upper = np.nan\n",
    "        p_value = np.nan # Set p_value to NaN as well\n",
    "        return pd.Series([lor, lor_ci_lower, lor_ci_upper, p_value], index=output_cols) # Return with p_value\n",
    "\n",
    "\n",
    "    # Calculate Standard Error of Log Odds Ratio\n",
    "    se_lor = math.sqrt(1/a_corr + 1/b_corr + 1/c_corr + 1/d_corr)\n",
    "\n",
    "    # Z-value for 95% CI (two-tailed)\n",
    "    z_value = 1.96\n",
    "\n",
    "    # Calculate LOR Confidence Interval\n",
    "    lor_ci_lower = lor - z_value * se_lor\n",
    "    lor_ci_upper = lor + z_value * se_lor\n",
    "\n",
    "    # Transform back to Odds Ratio Confidence Interval (not directly returned by your function but good to keep)\n",
    "    or_ci_lower = math.exp(lor_ci_lower)\n",
    "    or_ci_upper = math.exp(lor_ci_upper)\n",
    "\n",
    "    # *** Calculate the P-value ***\n",
    "    if se_lor == 0: # Avoid division by zero if SE is somehow 0 (unlikely with Haldane's but good practice)\n",
    "        p_value = np.nan\n",
    "    else:\n",
    "        z_statistic = lor / se_lor\n",
    "        # For a two-tailed test, we take 2 * (1 - CDF(abs(Z-statistic)))\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_statistic)))\n",
    "\n",
    "\n",
    "    return pd.Series([lor, np.round(lor_ci_lower, 3), np.round(lor_ci_upper, 3), np.round(p_value*3, 4)], # Rounded p_value, p_value x 2 to correct for multiple comparisons\n",
    "                     index=output_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "421145c1-5dee-4ae7-b4f4-d9677aed30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def df_to_complex_lor_latex_table(df: pd.DataFrame, caption: str = \"Logistic Odds Ratios and 95% Confidence Intervals\"):\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame into a specific LaTeX table format for LORs.\n",
    "\n",
    "    The input DataFrame should have the following columns:\n",
    "    - 'Category': Optional string, e.g., 'Impact', 'Narrative'. Used for bolded headers.\n",
    "    - 'Item': String, the descriptive item name for each row.\n",
    "    - 'Spiritual%': Float, percentage for Spiritual group.\n",
    "    - 'Psychosis%': Float, percentage for Psychosis group.\n",
    "    - 'Ayahuasca%': Float, percentage for Ayahuasca group.\n",
    "    - '1_LOR', '1_LOR_CI_Lower', '1_LOR_CI_Upper', '1_p_val': Data for comparison 1.\n",
    "    - '2_LOR', '2_LOR_CI_Lower', '2_LOR_CI_Upper', '2_p_val': Data for comparison 2.\n",
    "    - '3_LOR', '3_LOR_CI_Lower', '3_LOR_CI_Upper', '3_p_val': Data for comparison 3.\n",
    "\n",
    "    NaN values in the DataFrame will be represented by a hyphen '-'.\n",
    "    P-values less than 0.05 will add an asterisk (*) to the CI.\n",
    "    P-values less than 0.001 will be displayed as '<.001'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        caption (str): The caption for the LaTeX table.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the LaTeX table code.\n",
    "    \"\"\"\n",
    "    latex_code = []\n",
    "\n",
    "    # --- LaTeX table setup ---\n",
    "    latex_code.append(r\"\\begingroup % To limit the scope of \\renewcommand{\\arraystretch}\")\n",
    "    latex_code.append(r\"\\renewcommand{\\arraystretch}{1.2} % Adjust this value to control row spacing (increased for readability)\")\n",
    "    latex_code.append(\"\")\n",
    "    latex_code.append(r\"    \\centering\")\n",
    "    latex_code.append(r\"    \\begingroup\")\n",
    "    latex_code.append(r\"    \\footnotesize\") # Using footnotesize for a potentially wide table\n",
    "    latex_code.append(r\"    \\setlength{\\tabcolsep}{4pt}\") # Adjust column separation if needed\n",
    "    latex_code.append(r\"    \\setlength{\\extrarowheight}{1pt}\")\n",
    "\n",
    "    # --- Define tabular environment columns ---\n",
    "    # 1 for Item (p-column), 3 for percentages (S-columns), 3 sets of (LOR S, CI p, p S)\n",
    "    # Total columns: 1 + 3 + (3 * 3) = 13 columns\n",
    "    # Adjust p-column widths to accommodate content.\n",
    "    latex_code.append(r\"  \\begin{tabular}{p{4.0cm} c c c c c c c c c c c c }\")\n",
    "    # --- Table Headers ---\n",
    "    latex_code.append(r\"        \\toprule\")\n",
    "    # First header row (multi-column for comparisons)\n",
    "    latex_code.append(r\"        \\textbf{Item} & {\\textbf{Spiritual\\%}} & {\\textbf{Psychosis\\%}} & {\\textbf{Ayahuasca\\%}} & \\multicolumn{3}{c}{\\textbf{Comparison 1}} & \\multicolumn{3}{c}{\\textbf{Comparison 2}} & \\multicolumn{3}{c}{\\textbf{Comparison 3}} \\\\\")\n",
    "    latex_code.append(r\"        \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\") # Rules below multicolumns\n",
    "    # Second header row (specific metrics)\n",
    "    latex_code.append(r\"        & & & & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} \\\\\")\n",
    "    latex_code.append(r\"        \\midrule\")\n",
    "\n",
    "    previous_category = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        current_category = row.get('Category')\n",
    "        item_name = idx\n",
    "\n",
    "        # --- Handle Category Header ---\n",
    "        if pd.notna(current_category) and current_category != previous_category:\n",
    "            if previous_category is not None: # Add vertical space before new category if not the first\n",
    "                latex_code.append(r\"        [1ex]\")\n",
    "            latex_code.append(r\"        \\textbf{\" + str(current_category) + r\"} & & & & & & & & & & & & \\\\\") # Category row\n",
    "            previous_category = current_category\n",
    "            latex_code.append(r\"        [1ex]\") # Vertical space after category header\n",
    "\n",
    "        # --- Format Data for Current Row ---\n",
    "        # Item name (indented)\n",
    "        item_display = r\"\\hspace{1mm} \" + str(item_name) if pd.notna(item_name) else \"\"\n",
    "\n",
    "        # Percentages\n",
    "        spiritual_perc = f\"{row['Spiritual%']:.1f}\" if pd.notna(row.get('Spiritual%')) else '-'\n",
    "        psychosis_perc = f\"{row['Psychosis%']:.1f}\" if pd.notna(row.get('Psychosis%')) else '-'\n",
    "        ayahuasca_perc = f\"{row['Ayahuasca%']:.1f}\" if pd.notna(row.get('Ayahuasca%')) else '-'\n",
    "\n",
    "        # Function to format LOR, CI, and p-value for each comparison\n",
    "        def format_lor_ci_p(lor, ci_lower, ci_upper, p_val):\n",
    "            lor_str = f\"{lor:.3f}\" if pd.notna(lor) else '-'\n",
    "            \n",
    "            # P-value formatting\n",
    "            p_val_str = '-'\n",
    "            if pd.notna(p_val):\n",
    "                if p_val < 0.001:\n",
    "                    p_val_str = '<.001'\n",
    "                else:\n",
    "                    p_val_str = f\"{p_val:.3f}\"\n",
    "            \n",
    "            # CI formatting\n",
    "            ci_str = '-'\n",
    "            significance_asterisk = \"\"\n",
    "            if pd.notna(ci_lower) and pd.notna(ci_upper):\n",
    "                ci_str = f\"[{ci_lower:.3f}, {ci_upper:.3f}]\"\n",
    "                if pd.notna(p_val) and p_val < 0.05:\n",
    "                    significance_asterisk = \"*\"\n",
    "            \n",
    "            return lor_str, f\"{ci_str}{significance_asterisk}\", p_val_str\n",
    "\n",
    "        lor1_str, ci1_str, p1_str = format_lor_ci_p(\n",
    "            row.get('1_LOR'), row.get('1_LOR_CI_Lower'), row.get('1_LOR_CI_Upper'), row.get('1_p_val')\n",
    "        )\n",
    "        lor2_str, ci2_str, p2_str = format_lor_ci_p(\n",
    "            row.get('2_LOR'), row.get('2_LOR_CI_Lower'), row.get('2_LOR_CI_Upper'), row.get('2_p_val')\n",
    "        )\n",
    "        lor3_str, ci3_str, p3_str = format_lor_ci_p(\n",
    "            row.get('3_LOR'), row.get('3_LOR_CI_Lower'), row.get('3_LOR_CI_Upper'), row.get('3_p_val')\n",
    "        )\n",
    "\n",
    "        # --- Assemble LaTeX row ---\n",
    "        latex_row = (\n",
    "            f\"{item_display} & \"\n",
    "            f\"{spiritual_perc} & \"\n",
    "            f\"{psychosis_perc} & \"\n",
    "            f\"{ayahuasca_perc} & \"\n",
    "            f\"{lor1_str} & {ci1_str} & {p1_str} & \"\n",
    "            f\"{lor2_str} & {ci2_str} & {p2_str} & \"\n",
    "            f\"{lor3_str} & {ci3_str} & {p3_str} \\\\\\\\\"\n",
    "        )\n",
    "        latex_code.append(latex_row)\n",
    "\n",
    "    latex_code.append(r\"        \\bottomrule\")\n",
    "    latex_code.append(r\"    \\end{tabular}\")\n",
    "    latex_code.append(f\"    \\\\caption{{\\\\textbf{{{caption}}}}}\")\n",
    "    latex_code.append(r\"    \\label{tab:lor_summary}\") # Add a specific label\n",
    "    latex_code.append(r\"\\end{table}\")\n",
    "    latex_code.append(r\"\\endgroup{}\") # Close the initial \\begingroup\n",
    "\n",
    "    return \"\\n\".join(latex_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7dcf52e-fe10-4924-845b-b675439042b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begingroup % To limit the scope of \\renewcommand{\\arraystretch}\n",
      "\\renewcommand{\\arraystretch}{1.2} % Adjust this value to control row spacing (increased for readability)\n",
      "\n",
      "\\begin{table}[h!] % Using [h!] to prioritize placement\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\tabcolsep}{4pt}\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "    \\begin{tabular}{p{4.0cm} S[table-format=3.1] S[table-format=3.1] S[table-format=3.1] S[table-format=-1.3] p{2.2cm} S[table-format=1.3] S[table-format=-1.3] p{2.2cm} S[table-format=1.3] S[table-format=-1.3] p{2.2cm} S[table-format=1.3]}\n",
      "        \\toprule\n",
      "        \\textbf{Item} & {\\textbf{Spiritual\\%}} & {\\textbf{Psychosis\\%}} & {\\textbf{Ayahuasca\\%}} & \\multicolumn{3}{c}{\\textbf{Comparison 1}} & \\multicolumn{3}{c}{\\textbf{Comparison 2}} & \\multicolumn{3}{c}{\\textbf{Comparison 3}} \\\\\n",
      "        \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        & & & & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} \\\\\n",
      "        \\midrule\n",
      "\\hspace{1mm} Supernatural narrative & 100.0 & 20.0 & 100.0 & 5.311 & [2.413, 8.209]* & 0.001 & -0.424 & [-4.375, 3.526] & 2.500 & -5.758 & [-8.700, -2.816]* & <.001 \\\\\n",
      "\\hspace{1mm} Self-stigma & 30.8 & 47.5 & 7.1 & -0.680 & [-1.698, 0.337] & 0.571 & 1.594 & [0.230, 2.958] & 0.066 & 2.223 & [0.892, 3.554]* & 0.003 \\\\\n",
      "\\hspace{1mm} Negative on relationships & 26.9 & 77.5 & 7.1 & -2.154 & [-3.266, -1.043]* & <.001 & 1.416 & [0.034, 2.798] & 0.134 & 3.520 & [2.114, 4.927]* & <.001 \\\\\n",
      "\\hspace{1mm} Biophysical narrative & 23.1 & 25.0 & 28.6 & -0.082 & [-1.209, 1.045] & 2.659 & -0.207 & [-1.321, 0.908] & 2.149 & -0.207 & [-1.321, 0.908] & 2.149 \\\\\n",
      "\\hspace{1mm} Family narrative & 19.2 & 22.5 & 0.0 & -0.165 & [-1.349, 1.020] & 2.356 & 3.031 & [0.089, 5.973] & 0.130 & 3.246 & [0.321, 6.171] & 0.089 \\\\\n",
      "\\hspace{1mm} Positive on relationships & 15.4 & 5.0 & 50.0 & 1.125 & [-0.506, 2.756] & 0.529 & -1.609 & [-2.792, -0.427]* & 0.023 & -2.833 & [-4.590, -1.076]* & 0.005 \\\\\n",
      "\\hspace{1mm} Idiosyncratic narrative & 11.5 & 32.5 & 42.9 & -1.193 & [-2.491, 0.105] & 0.215 & -1.609 & [-2.892, -0.327]* & 0.042 & -0.483 & [-1.504, 0.538] & 1.062 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Comparison of Groups based on Logistic Odds Ratios}}\n",
      "    \\label{tab:lor_summary}\n",
      "    \\endgroup\n",
      "\\end{table}\n",
      "\\endgroup{}\n"
     ]
    }
   ],
   "source": [
    "#For overall codes\n",
    "#compute the log odds for the spiritualists and vips as a sanity check\n",
    "df_both[['1_LOR','1_LOR_CI_Lower', '1_LOR_CI_Upper', '1_p_val']] = df_both.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Psychosis%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_both[['2_LOR','2_LOR_CI_Lower', '2_LOR_CI_Upper', '2_p_val']] = df_both.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_both[['3_LOR','3_LOR_CI_Lower', '3_LOR_CI_Upper', '3_p_val']] = df_both.apply(lambda row : calculate_lor_and_ci(row, 'Psychosis%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "df_both\n",
    "\n",
    "latex_output = df_to_complex_lor_latex_table(df_both, caption=\"Comparison of Groups based on Logistic Odds Ratios\")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8204696-84a4-4628-bccb-3cba910bec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begingroup % To limit the scope of \\renewcommand{\\arraystretch}\n",
      "\\renewcommand{\\arraystretch}{1.2} % Adjust this value to control row spacing (increased for readability)\n",
      "\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\tabcolsep}{4pt}\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "    \\begin{tabular}{p{4.0cm} S[table-format=3.1] S[table-format=3.1] S[table-format=3.1] S[table-format=-1.3] p{2.2cm} S[table-format=1.3] S[table-format=-1.3] p{2.2cm} S[table-format=1.3] S[table-format=-1.3] p{2.2cm} S[table-format=1.3]}\n",
      "        \\toprule\n",
      "        \\textbf{Item} & {\\textbf{Spiritual\\%}} & {\\textbf{Psychosis\\%}} & {\\textbf{Ayahuasca\\%}} & \\multicolumn{3}{c}{\\textbf{Comparison 1}} & \\multicolumn{3}{c}{\\textbf{Comparison 2}} & \\multicolumn{3}{c}{\\textbf{Comparison 3}} \\\\\n",
      "        \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        & & & & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} \\\\\n",
      "        \\midrule\n",
      "\\hspace{1mm} Visual imagery & 100.0 & 5.0 & 85.7 & 6.705 & [3.628, 9.781]* & <.001 & 2.301 & [-0.620, 5.222] & 0.368 & -4.502 & [-6.350, -2.655]* & <.001 \\\\\n",
      "\\hspace{1mm} Auditory & 84.6 & 92.5 & 42.9 & -0.762 & [-2.254, 0.729] & 0.950 & 1.904 & [0.718, 3.091]* & 0.005 & 2.577 & [1.136, 4.018]* & 0.001 \\\\\n",
      "\\hspace{1mm} Internally located & 84.6 & 62.5 & 92.9 & 1.112 & [-0.081, 2.304] & 0.203 & -0.762 & [-2.254, 0.729] & 0.950 & -1.920 & [-3.261, -0.578]* & 0.015 \\\\\n",
      "\\hspace{1mm} Egocentric & 80.8 & 70.0 & 0.0 & 0.539 & [-0.609, 1.688] & 1.073 & 5.758 & [2.816, 8.700]* & <.001 & 5.172 & [2.267, 8.077]* & 0.001 \\\\\n",
      "\\hspace{1mm} Thought-like & 76.9 & 52.5 & 100.0 & 1.051 & [-0.023, 2.125] & 0.166 & -3.246 & [-6.171, -0.321] & 0.089 & -4.246 & [-7.136, -1.356]* & 0.012 \\\\\n",
      "\\hspace{1mm} Externally located & 73.1 & 80.0 & 42.9 & -0.386 & [-1.517, 0.745] & 1.512 & 1.250 & [0.205, 2.295] & 0.057 & 1.658 & [0.536, 2.781]* & 0.011 \\\\\n",
      "\\hspace{1mm} Felt presence & 69.2 & 52.5 & 35.7 & 0.680 & [-0.337, 1.698] & 0.571 & 1.381 & [0.347, 2.415]* & 0.027 & 0.751 & [-0.240, 1.742] & 0.412 \\\\\n",
      "\\hspace{1mm} Multimodal & 69.2 & 27.5 & 85.7 & 1.720 & [0.660, 2.780]* & 0.004 & -0.891 & [-2.058, 0.276] & 0.403 & -2.625 & [-3.813, -1.437]* & <.001 \\\\\n",
      "\\hspace{1mm} Visual & 60.0 & 75.0 & 0.0 & -0.614 & [-1.659, 0.430] & 0.747 & 4.846 & [1.952, 7.741]* & 0.003 & 5.543 & [2.618, 8.468]* & <.001 \\\\\n",
      "\\hspace{1mm} Bodily states & 53.8 & 65.0 & 78.6 & -0.455 & [-1.446, 0.536] & 1.106 & -1.050 & [-2.098, -0.002] & 0.149 & -0.588 & [-1.660, 0.485] & 0.848 \\\\\n",
      "\\hspace{1mm} Olfactory & 50.0 & 37.5 & 7.1 & 0.498 & [-0.486, 1.482] & 0.964 & 2.372 & [1.041, 3.702]* & 0.001 & 1.920 & [0.578, 3.261]* & 0.015 \\\\\n",
      "\\hspace{1mm} Dissociative & 46.2 & 30.0 & 7.1 & 0.676 & [-0.331, 1.683] & 0.566 & 2.223 & [0.892, 3.554]* & 0.003 & 1.594 & [0.230, 2.958] & 0.066 \\\\\n",
      "\\hspace{1mm} Tactile & 46.2 & 22.5 & 7.1 & 1.050 & [0.002, 2.098] & 0.149 & 2.223 & [0.892, 3.554]* & 0.003 & 1.223 & [-0.184, 2.630] & 0.265 \\\\\n",
      "\\hspace{1mm} Gustatory & 26.9 & 0.0 & 0.0 & 3.439 & [0.526, 6.352] & 0.062 & 3.439 & [0.526, 6.352] & 0.062 & 0.424 & [-3.526, 4.375] & 2.500 \\\\\n",
      "\\hspace{1mm} Nonverbal & 26.9 & 40.0 & 0.0 & -0.560 & [-1.608, 0.488] & 0.885 & 3.439 & [0.526, 6.352] & 0.062 & 3.942 & [1.048, 6.837]* & 0.023 \\\\\n",
      "\\hspace{1mm} Allocentric & 23.1 & 37.5 & 0.0 & -0.651 & [-1.736, 0.434] & 0.719 & 3.246 & [0.321, 6.171] & 0.089 & 3.942 & [1.048, 6.837]* & 0.023 \\\\\n",
      "\\hspace{1mm} Boundary & 11.5 & 35.0 & 0.0 & -1.301 & [-2.594, -0.009] & 0.145 & 2.490 & [-0.516, 5.497] & 0.314 & 3.784 & [0.885, 6.682]* & 0.032 \\\\\n",
      "\\hspace{1mm} Elicit positive emotions & 100.0 & 35.0 & 92.9 & 4.573 & [1.703, 7.443]* & 0.005 & 1.599 & [-1.406, 4.603] & 0.891 & -2.982 & [-4.333, -1.632]* & <.001 \\\\\n",
      "\\hspace{1mm} Voice knowledge & 96.2 & 45.0 & 92.9 & 3.029 & [1.271, 4.787]* & 0.002 & 0.462 & [-1.516, 2.439] & 1.942 & -2.520 & [-3.851, -1.189]* & <.001 \\\\\n",
      "\\hspace{1mm} Volitional & 92.3 & 2.5 & 21.4 & 5.553 & [3.467, 7.639]* & <.001 & 3.481 & [1.991, 4.971]* & <.001 & -1.635 & [-3.434, 0.165] & 0.225 \\\\\n",
      "\\hspace{1mm} Positive/helpful & 92.3 & 42.5 & 100.0 & 2.577 & [1.136, 4.018]* & 0.001 & -2.112 & [-5.190, 0.966] & 0.536 & -4.693 & [-7.584, -1.802]* & 0.004 \\\\\n",
      "\\hspace{1mm} Recurring & 92.3 & 92.5 & 92.9 & -0.089 & [-1.790, 1.612] & 2.754 & -0.089 & [-1.790, 1.612] & 2.754 & -0.089 & [-1.790, 1.612] & 2.754 \\\\\n",
      "\\hspace{1mm} Ability to influence & 84.6 & 27.5 & 50.0 & 2.551 & [1.331, 3.772]* & <.001 & 1.609 & [0.427, 2.792]* & 0.023 & -0.956 & [-1.997, 0.086] & 0.216 \\\\\n",
      "\\hspace{1mm} Change in frequency & 80.8 & 70.0 & 21.4 & 0.539 & [-0.609, 1.688] & 1.073 & 2.562 & [1.377, 3.747]* & <.001 & 1.976 & [0.887, 3.065]* & 0.001 \\\\\n",
      "\\hspace{1mm} Recognizable & 76.9 & 47.5 & 85.7 & 1.246 & [0.172, 2.321] & 0.069 & -0.521 & [-1.737, 0.696] & 1.205 & -1.818 & [-2.947, -0.689]* & 0.005 \\\\\n",
      "\\hspace{1mm} Change in influence & 73.1 & 12.5 & 35.7 & 2.820 & [1.589, 4.052]* & <.001 & 1.559 & [0.501, 2.616]* & 0.012 & -1.301 & [-2.594, -0.009] & 0.145 \\\\\n",
      "\\hspace{1mm} Simple structure & 69.2 & 40.0 & 42.9 & 1.173 & [0.149, 2.197] & 0.074 & 1.073 & [0.051, 2.094] & 0.119 & -0.157 & [-1.148, 0.834] & 2.268 \\\\\n",
      "\\hspace{1mm} Conversational & 65.4 & 47.5 & 64.3 & 0.709 & [-0.291, 1.708] & 0.494 & 0.008 & [-1.009, 1.025] & 2.963 & -0.751 & [-1.742, 0.240] & 0.412 \\\\\n",
      "\\hspace{1mm} Direct address & 61.5 & 82.5 & 85.7 & -1.045 & [-2.152, 0.062] & 0.193 & -1.217 & [-2.358, -0.077] & 0.109 & -0.306 & [-1.563, 0.951] & 1.900 \\\\\n",
      "\\hspace{1mm} Companionship & 42.3 & 32.5 & 28.6 & 0.413 & [-0.590, 1.416] & 1.259 & 0.644 & [-0.379, 1.666] & 0.652 & 0.164 & [-0.896, 1.224] & 2.284 \\\\\n",
      "\\hspace{1mm} Elicit negative emotions & 38.5 & 100.0 & 71.4 & -4.846 & [-7.741, -1.952]* & 0.003 & -1.394 & [-2.425, -0.363]* & 0.024 & 3.028 & [0.149, 5.908] & 0.118 \\\\\n",
      "\\hspace{1mm} Commanding & 26.9 & 67.5 & 78.6 & -1.667 & [-2.732, -0.602]* & 0.006 & -2.154 & [-3.266, -1.043]* & <.001 & -0.421 & [-1.510, 0.668] & 1.346 \\\\\n",
      "\\hspace{1mm} Commenting & 19.2 & 45.0 & 64.3 & -1.168 & [-2.288, -0.047] & 0.123 & -1.966 & [-3.101, -0.832]* & 0.002 & -0.751 & [-1.742, 0.240] & 0.412 \\\\\n",
      "\\hspace{1mm} Character change & 11.5 & 17.5 & 21.4 & -0.408 & [-1.782, 0.966] & 1.683 & -0.706 & [-2.042, 0.631] & 0.903 & -0.165 & [-1.349, 1.020] & 2.356 \\\\\n",
      "\\hspace{1mm} Abusive/violent & 11.5 & 87.5 & 14.3 & -3.769 & [-5.207, -2.331]* & <.001 & -0.235 & [-1.636, 1.166] & 2.227 & 3.573 & [2.172, 4.975]* & <.001 \\\\\n",
      "\\hspace{1mm} Supernatural narrative & 100.0 & 20.0 & 100.0 & 5.311 & [2.413, 8.209]* & 0.001 & -0.424 & [-4.375, 3.526] & 2.500 & -5.758 & [-8.700, -2.816]* & <.001 \\\\\n",
      "\\hspace{1mm} Internally individualized & 100.0 & 75.0 & 0.0 & 2.904 & [0.020, 5.788] & 0.145 & 8.365 & [4.414, 12.315]* & <.001 & 5.543 & [2.618, 8.468]* & <.001 \\\\\n",
      "\\hspace{1mm} Externally individualized & 80.8 & 50.0 & 100.0 & 1.363 & [0.244, 2.482] & 0.051 & -3.031 & [-5.973, -0.089] & 0.130 & -4.394 & [-7.284, -1.505]* & 0.009 \\\\\n",
      "\\hspace{1mm} Minimal personification & 61.5 & 60.0 & 28.6 & 0.057 & [-0.937, 1.051] & 2.733 & 1.394 & [0.363, 2.425]* & 0.024 & 1.394 & [0.363, 2.425]* & 0.024 \\\\\n",
      "\\hspace{1mm} Complex personification & 38.5 & 40.0 & 78.6 & -0.057 & [-1.051, 0.937] & 2.733 & -1.651 & [-2.711, -0.590]* & 0.007 & -1.651 & [-2.711, -0.590]* & 0.007 \\\\\n",
      "\\hspace{1mm} Agency w/o individuation & 38.5 & 45.0 & 0.0 & -0.256 & [-1.245, 0.732] & 1.834 & 3.942 & [1.048, 6.837]* & 0.023 & 4.246 & [1.356, 7.136]* & 0.012 \\\\\n",
      "\\hspace{1mm} Self-stigma & 30.8 & 47.5 & 7.1 & -0.680 & [-1.698, 0.337] & 0.571 & 1.594 & [0.230, 2.958] & 0.066 & 2.223 & [0.892, 3.554]* & 0.003 \\\\\n",
      "\\hspace{1mm} Negative on relationships & 26.9 & 77.5 & 7.1 & -2.154 & [-3.266, -1.043]* & <.001 & 1.416 & [0.034, 2.798] & 0.134 & 3.520 & [2.114, 4.927]* & <.001 \\\\\n",
      "\\hspace{1mm} Biophysical narrative & 23.1 & 25.0 & 28.6 & -0.082 & [-1.209, 1.045] & 2.659 & -0.207 & [-1.321, 0.908] & 2.149 & -0.207 & [-1.321, 0.908] & 2.149 \\\\\n",
      "\\hspace{1mm} Archetypal & 23.1 & 42.5 & 85.7 & -0.854 & [-1.932, 0.224] & 0.361 & -2.818 & [-4.035, -1.601]* & <.001 & -1.968 & [-3.101, -0.834]* & 0.002 \\\\\n",
      "\\hspace{1mm} Family narrative & 19.2 & 22.5 & 0.0 & -0.165 & [-1.349, 1.020] & 2.356 & 3.031 & [0.089, 5.973] & 0.130 & 3.246 & [0.321, 6.171] & 0.089 \\\\\n",
      "\\hspace{1mm} Positive on relationships & 15.4 & 5.0 & 50.0 & 1.125 & [-0.506, 2.756] & 0.529 & -1.609 & [-2.792, -0.427]* & 0.023 & -2.833 & [-4.590, -1.076]* & 0.005 \\\\\n",
      "\\hspace{1mm} Idiosyncratic narrative & 11.5 & 32.5 & 42.9 & -1.193 & [-2.491, 0.105] & 0.215 & -1.609 & [-2.892, -0.327]* & 0.042 & -0.483 & [-1.504, 0.538] & 1.062 \\\\\n",
      "\\hspace{1mm} Absent agency & 11.5 & 15.0 & 7.1 & -0.235 & [-1.636, 1.166] & 2.227 & 0.467 & [-1.101, 2.036] & 1.678 & 0.762 & [-0.729, 2.254] & 0.950 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Comparison of Groups based on Logistic Odds Ratios}}\n",
      "    \\label{tab:lor_summary}\n",
      "\\end{table}\n",
      "\\endgroup{}\n"
     ]
    }
   ],
   "source": [
    "#For during codes\n",
    "#compute the log odds for the spiritualists and vips as a sanity check\n",
    "df_during[['1_LOR','1_LOR_CI_Lower', '1_LOR_CI_Upper', '1_p_val']] = df_during.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Psychosis%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_during[['2_LOR','2_LOR_CI_Lower', '2_LOR_CI_Upper', '2_p_val']] = df_during.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_during[['3_LOR','3_LOR_CI_Lower', '3_LOR_CI_Upper', '3_p_val']] = df_during.apply(lambda row : calculate_lor_and_ci(row, 'Psychosis%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "df_during\n",
    "latex_output = df_to_complex_lor_latex_table(df_during, caption=\"Comparison of Groups based on Logistic Odds Ratios\")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "187573c6-66e2-4601-804b-2b388df84cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begingroup % To limit the scope of \\renewcommand{\\arraystretch}\n",
      "\\renewcommand{\\arraystretch}{1.2} % Adjust this value to control row spacing (increased for readability)\n",
      "\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\tabcolsep}{4pt}\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "  \\begin{tabular}{p{4.0cm} c c c c c c c c c c c c }\n",
      "        \\toprule\n",
      "        \\textbf{Item} & {\\textbf{Spiritual\\%}} & {\\textbf{Psychosis\\%}} & {\\textbf{Ayahuasca\\%}} & \\multicolumn{3}{c}{\\textbf{Comparison 1}} & \\multicolumn{3}{c}{\\textbf{Comparison 2}} & \\multicolumn{3}{c}{\\textbf{Comparison 3}} \\\\\n",
      "        \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        & & & & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} & {\\textbf{LOR}} & {\\textbf{95\\% CI}} & {\\textbf{\\textit{p}}} \\\\\n",
      "        \\midrule\n",
      "\\hspace{1mm} Visual imagery & 100.0 & 5.0 & 57.1 & 6.705 & [3.628, 9.781]* & <.001 & 3.675 & [0.810, 6.541]* & 0.036 & -3.128 & [-4.887, -1.369]* & 0.002 \\\\\n",
      "\\hspace{1mm} Auditory & 84.6 & 92.5 & 7.1 & -0.762 & [-2.254, 0.729] & 0.950 & 3.981 & [2.490, 5.472]* & <.001 & 4.654 & [2.953, 6.355]* & <.001 \\\\\n",
      "\\hspace{1mm} Internally located & 84.6 & 62.5 & 85.7 & 1.112 & [-0.081, 2.304] & 0.203 & -0.060 & [-1.374, 1.254] & 2.787 & -1.217 & [-2.358, -0.077] & 0.109 \\\\\n",
      "\\hspace{1mm} Egocentric & 80.8 & 70.0 & 7.1 & 0.539 & [-0.609, 1.688] & 1.073 & 3.735 & [2.294, 5.176]* & <.001 & 3.149 & [1.786, 4.513]* & <.001 \\\\\n",
      "\\hspace{1mm} Thought-like & 76.9 & 52.5 & 100.0 & 1.051 & [-0.023, 2.125] & 0.166 & -3.246 & [-6.171, -0.321] & 0.089 & -4.246 & [-7.136, -1.356]* & 0.012 \\\\\n",
      "\\hspace{1mm} Externally located & 73.1 & 80.0 & 28.6 & -0.386 & [-1.517, 0.745] & 1.512 & 1.898 & [0.814, 2.981]* & 0.002 & 2.305 & [1.147, 3.464]* & <.001 \\\\\n",
      "\\hspace{1mm} Felt presence & 69.2 & 52.5 & 64.3 & 0.680 & [-0.337, 1.698] & 0.571 & 0.175 & [-0.859, 1.209] & 2.222 & -0.455 & [-1.446, 0.536] & 1.106 \\\\\n",
      "\\hspace{1mm} Multimodal & 69.2 & 27.5 & 71.4 & 1.720 & [0.660, 2.780]* & 0.004 & -0.164 & [-1.224, 0.896] & 2.284 & -1.898 & [-2.981, -0.814]* & 0.002 \\\\\n",
      "\\hspace{1mm} Visual & 60.0 & 75.0 & 14.3 & -0.614 & [-1.659, 0.430] & 0.747 & 2.121 & [0.981, 3.262]* & <.001 & 2.818 & [1.601, 4.035]* & <.001 \\\\\n",
      "\\hspace{1mm} Bodily states & 53.8 & 65.0 & 57.1 & -0.455 & [-1.446, 0.536] & 1.106 & -0.146 & [-1.124, 0.831] & 2.307 & 0.316 & [-0.687, 1.320] & 1.611 \\\\\n",
      "\\hspace{1mm} Olfactory & 50.0 & 37.5 & 0.0 & 0.498 & [-0.486, 1.482] & 0.964 & 4.394 & [1.505, 7.284]* & 0.009 & 3.942 & [1.048, 6.837]* & 0.023 \\\\\n",
      "\\hspace{1mm} Dissociative & 46.2 & 30.0 & 0.0 & 0.676 & [-0.331, 1.683] & 0.566 & 4.246 & [1.356, 7.136]* & 0.012 & 3.617 & [0.712, 6.522]* & 0.044 \\\\\n",
      "\\hspace{1mm} Tactile & 46.2 & 22.5 & 7.1 & 1.050 & [0.002, 2.098] & 0.149 & 2.223 & [0.892, 3.554]* & 0.003 & 1.223 & [-0.184, 2.630] & 0.265 \\\\\n",
      "\\hspace{1mm} Gustatory & 26.9 & 0.0 & 0.0 & 3.439 & [0.526, 6.352] & 0.062 & 3.439 & [0.526, 6.352] & 0.062 & 0.424 & [-3.526, 4.375] & 2.500 \\\\\n",
      "\\hspace{1mm} Nonverbal & 26.9 & 40.0 & 0.0 & -0.560 & [-1.608, 0.488] & 0.885 & 3.439 & [0.526, 6.352] & 0.062 & 3.942 & [1.048, 6.837]* & 0.023 \\\\\n",
      "\\hspace{1mm} Allocentric & 23.1 & 37.5 & 0.0 & -0.651 & [-1.736, 0.434] & 0.719 & 3.246 & [0.321, 6.171] & 0.089 & 3.942 & [1.048, 6.837]* & 0.023 \\\\\n",
      "\\hspace{1mm} Boundary & 11.5 & 35.0 & 0.0 & -1.301 & [-2.594, -0.009] & 0.145 & 2.490 & [-0.516, 5.497] & 0.314 & 3.784 & [0.885, 6.682]* & 0.032 \\\\\n",
      "\\hspace{1mm} Elicit positive emotions & 100.0 & 35.0 & 92.9 & 4.573 & [1.703, 7.443]* & 0.005 & 1.599 & [-1.406, 4.603] & 0.891 & -2.982 & [-4.333, -1.632]* & <.001 \\\\\n",
      "\\hspace{1mm} Voice knowledge & 96.2 & 45.0 & 85.7 & 3.029 & [1.271, 4.787]* & 0.002 & 1.164 & [-0.684, 3.012] & 0.651 & -1.818 & [-2.947, -0.689]* & 0.005 \\\\\n",
      "\\hspace{1mm} Volitional & 92.3 & 2.5 & 64.3 & 5.553 & [3.467, 7.639]* & <.001 & 1.679 & [0.229, 3.130] & 0.070 & -3.436 & [-5.203, -1.669]* & <.001 \\\\\n",
      "\\hspace{1mm} Positive/helpful & 92.3 & 42.5 & 92.9 & 2.577 & [1.136, 4.018]* & 0.001 & -0.089 & [-1.790, 1.612] & 2.754 & -2.670 & [-4.005, -1.335]* & <.001 \\\\\n",
      "\\hspace{1mm} Recurring & 92.3 & 92.5 & 85.7 & -0.089 & [-1.790, 1.612] & 2.754 & 0.613 & [-0.935, 2.161] & 1.312 & 0.613 & [-0.935, 2.161] & 1.312 \\\\\n",
      "\\hspace{1mm} Ability to influence & 84.6 & 27.5 & 85.7 & 2.551 & [1.331, 3.772]* & <.001 & -0.060 & [-1.374, 1.254] & 2.787 & -2.625 & [-3.813, -1.437]* & <.001 \\\\\n",
      "\\hspace{1mm} Change in frequency & 80.8 & 70.0 & 50.0 & 0.539 & [-0.609, 1.688] & 1.073 & 1.363 & [0.244, 2.482] & 0.051 & 0.778 & [-0.239, 1.795] & 0.402 \\\\\n",
      "\\hspace{1mm} Recognizable & 76.9 & 47.5 & 92.9 & 1.246 & [0.172, 2.321] & 0.069 & -1.223 & [-2.630, 0.184] & 0.265 & -2.520 & [-3.851, -1.189]* & <.001 \\\\\n",
      "\\hspace{1mm} Change in influence & 73.1 & 12.5 & 78.6 & 2.820 & [1.589, 4.052]* & <.001 & -0.243 & [-1.355, 0.868] & 2.004 & -3.103 & [-4.440, -1.766]* & <.001 \\\\\n",
      "\\hspace{1mm} Simple structure & 69.2 & 40.0 & 14.3 & 1.173 & [0.149, 2.197] & 0.074 & 2.447 & [1.280, 3.614]* & <.001 & 1.217 & [0.077, 2.358] & 0.109 \\\\\n",
      "\\hspace{1mm} Conversational & 65.4 & 47.5 & 42.9 & 0.709 & [-0.291, 1.708] & 0.494 & 0.906 & [-0.098, 1.909] & 0.231 & 0.146 & [-0.831, 1.124] & 2.307 \\\\\n",
      "\\hspace{1mm} Direct address & 61.5 & 82.5 & 78.6 & -1.045 & [-2.152, 0.062] & 0.193 & -0.747 & [-1.807, 0.314] & 0.503 & 0.165 & [-1.020, 1.349] & 2.356 \\\\\n",
      "\\hspace{1mm} Companionship & 42.3 & 32.5 & 28.6 & 0.413 & [-0.590, 1.416] & 1.259 & 0.644 & [-0.379, 1.666] & 0.652 & 0.164 & [-0.896, 1.224] & 2.284 \\\\\n",
      "\\hspace{1mm} Elicit negative emotions & 38.5 & 100.0 & 57.1 & -4.846 & [-7.741, -1.952]* & 0.003 & -0.747 & [-1.738, 0.244] & 0.419 & 3.675 & [0.810, 6.541]* & 0.036 \\\\\n",
      "\\hspace{1mm} Commanding & 26.9 & 67.5 & 71.4 & -1.667 & [-2.732, -0.602]* & 0.006 & -1.898 & [-2.981, -0.814]* & 0.002 & -0.164 & [-1.224, 0.896] & 2.284 \\\\\n",
      "\\hspace{1mm} Commenting & 19.2 & 45.0 & 57.1 & -1.168 & [-2.288, -0.047] & 0.123 & -1.658 & [-2.781, -0.536]* & 0.011 & -0.443 & [-1.421, 0.534] & 1.122 \\\\\n",
      "\\hspace{1mm} Character change & 11.5 & 17.5 & 7.1 & -0.408 & [-1.782, 0.966] & 1.683 & 0.467 & [-1.101, 2.036] & 1.678 & 1.008 & [-0.433, 2.450] & 0.511 \\\\\n",
      "\\hspace{1mm} Abusive/violent & 11.5 & 87.5 & 21.4 & -3.769 & [-5.207, -2.331]* & <.001 & -0.706 & [-2.042, 0.631] & 0.903 & 3.103 & [1.766, 4.440]* & <.001 \\\\\n",
      "\\hspace{1mm} Supernatural narrative & 100.0 & 20.0 & 100.0 & 5.311 & [2.413, 8.209]* & 0.001 & -0.424 & [-4.375, 3.526] & 2.500 & -5.758 & [-8.700, -2.816]* & <.001 \\\\\n",
      "\\hspace{1mm} Internally individualized & 100.0 & 75.0 & 7.1 & 2.904 & [0.020, 5.788] & 0.145 & 6.342 & [3.337, 9.347]* & <.001 & 3.520 & [2.114, 4.927]* & <.001 \\\\\n",
      "\\hspace{1mm} Externally individualized & 80.8 & 50.0 & 85.7 & 1.363 & [0.244, 2.482] & 0.051 & -0.306 & [-1.563, 0.951] & 1.900 & -1.669 & [-2.797, -0.542]* & 0.011 \\\\\n",
      "\\hspace{1mm} Minimal personification & 61.5 & 60.0 & 21.4 & 0.057 & [-0.937, 1.051] & 2.733 & 1.651 & [0.590, 2.711]* & 0.007 & 1.651 & [0.590, 2.711]* & 0.007 \\\\\n",
      "\\hspace{1mm} Complex personification & 38.5 & 40.0 & 78.6 & -0.057 & [-1.051, 0.937] & 2.733 & -1.651 & [-2.711, -0.590]* & 0.007 & -1.651 & [-2.711, -0.590]* & 0.007 \\\\\n",
      "\\hspace{1mm} Agency w/o individuation & 38.5 & 45.0 & 7.1 & -0.256 & [-1.245, 0.732] & 1.834 & 1.920 & [0.578, 3.261]* & 0.015 & 2.223 & [0.892, 3.554]* & 0.003 \\\\\n",
      "\\hspace{1mm} Self-stigma & 30.8 & 47.5 & 7.1 & -0.680 & [-1.698, 0.337] & 0.571 & 1.594 & [0.230, 2.958] & 0.066 & 2.223 & [0.892, 3.554]* & 0.003 \\\\\n",
      "\\hspace{1mm} Negative on relationships & 26.9 & 77.5 & 7.1 & -2.154 & [-3.266, -1.043]* & <.001 & 1.416 & [0.034, 2.798] & 0.134 & 3.520 & [2.114, 4.927]* & <.001 \\\\\n",
      "\\hspace{1mm} Biophysical narrative & 23.1 & 25.0 & 28.6 & -0.082 & [-1.209, 1.045] & 2.659 & -0.207 & [-1.321, 0.908] & 2.149 & -0.207 & [-1.321, 0.908] & 2.149 \\\\\n",
      "\\hspace{1mm} Archetypal & 23.1 & 42.5 & 57.1 & -0.854 & [-1.932, 0.224] & 0.361 & -1.443 & [-2.521, -0.366]* & 0.026 & -0.593 & [-1.576, 0.389] & 0.710 \\\\\n",
      "\\hspace{1mm} Family narrative & 19.2 & 22.5 & 0.0 & -0.165 & [-1.349, 1.020] & 2.356 & 3.031 & [0.089, 5.973] & 0.130 & 3.246 & [0.321, 6.171] & 0.089 \\\\\n",
      "\\hspace{1mm} Positive on relationships & 15.4 & 5.0 & 50.0 & 1.125 & [-0.506, 2.756] & 0.529 & -1.609 & [-2.792, -0.427]* & 0.023 & -2.833 & [-4.590, -1.076]* & 0.005 \\\\\n",
      "\\hspace{1mm} Idiosyncratic narrative & 11.5 & 32.5 & 42.9 & -1.193 & [-2.491, 0.105] & 0.215 & -1.609 & [-2.892, -0.327]* & 0.042 & -0.483 & [-1.504, 0.538] & 1.062 \\\\\n",
      "\\hspace{1mm} Absent agency & 11.5 & 15.0 & 0.0 & -0.235 & [-1.636, 1.166] & 2.227 & 2.490 & [-0.516, 5.497] & 0.314 & 2.785 & [-0.182, 5.752] & 0.197 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Comparison of Groups based on Logistic Odds Ratios}}\n",
      "    \\label{tab:lor_summary}\n",
      "\\end{table}\n",
      "\\endgroup{}\n"
     ]
    }
   ],
   "source": [
    "#For overall codes\n",
    "#compute the log odds for the spiritualists and vips as a sanity check\n",
    "df_after[['1_LOR','1_LOR_CI_Lower', '1_LOR_CI_Upper', '1_p_val']] = df_after.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Psychosis%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_after[['2_LOR','2_LOR_CI_Lower', '2_LOR_CI_Upper', '2_p_val']] = df_after.apply(lambda row : calculate_lor_and_ci(row, 'Spiritual%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "#for the ayahuasca data we have, compute it too\n",
    "df_after[['3_LOR','3_LOR_CI_Lower', '3_LOR_CI_Upper', '3_p_val']] = df_after.apply(lambda row : calculate_lor_and_ci(row, 'Psychosis%', 'Ayahuasca%'), axis=1)\n",
    "\n",
    "df_after\n",
    "latex_output = df_to_complex_lor_latex_table(df_after, caption=\"Comparison of Groups based on Logistic Odds Ratios\")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d5106-2712-4aeb-9cde-221485e80b51",
   "metadata": {},
   "source": [
    "#### An odds ratio (OR) is statistically significant when its confidence interval (CI) does not include 1, indicating a difference in the odds of the event between the two groups being compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef7ebdaf-aca4-4c09-ba12-ec51815a7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_df_by_strict_zero_crossing(df: pd.DataFrame, col1_name: str, col2_name: str, p_value_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters a DataFrame, keeping only rows where both values are strictly positive\n",
    "    or both values are strictly negative. This ensures the interval does not\n",
    "    include or cross zero.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1_name (str): The name of the first numerical column.\n",
    "        col2_name (str): The name of the second numerical column.\n",
    "        p_value_col_name (str): The name of the column containing the p-values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    if col1_name not in df.columns or col2_name not in df.columns:\n",
    "        raise ValueError(f\"One or both columns '{col1_name}' or '{col2_name}' not found in the DataFrame.\")\n",
    "    \n",
    "    keep_condition = (df[p_value_col_name] < 0.05/2) # Add the p-value check here\n",
    "\n",
    "    df_filtered = df[keep_condition]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fbc12-fd18-47db-8d7e-31b4b0f43bab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79757749-2f7a-4b30-b402-650e63e21f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common elements\n",
      "['Auditory', 'Olfactory', 'Minimal personification', 'Complex personification', 'Visual', 'Externally located', 'Internally individualized', 'Positive on relationships', 'Egocentric', 'Agency w/o individuation'] \n",
      "\n",
      "comp1:  ['Auditory', 'Egocentric', 'Externally located', 'Visual', 'Olfactory', 'Dissociative', 'Tactile', 'Simple structure', 'Commanding', 'Commenting', 'Internally individualized', 'Minimal personification', 'Complex personification', 'Agency w/o individuation', 'Positive on relationships'] \n",
      "\n",
      "comp2:  ['Visual imagery', 'Auditory', 'Egocentric', 'Thought-like', 'Externally located', 'Multimodal', 'Visual', 'Olfactory', 'Nonverbal', 'Allocentric', 'Elicit positive emotions', 'Voice knowledge', 'Volitional', 'Positive/helpful', 'Ability to influence', 'Recognizable', 'Change in influence', 'Abusive/violent', 'Supernatural narrative', 'Internally individualized', 'Externally individualized', 'Minimal personification', 'Complex personification', 'Agency w/o individuation', 'Self-stigma', 'Negative on relationships', 'Positive on relationships'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spiritual%</th>\n",
       "      <th>Psychosis%</th>\n",
       "      <th>Ayahuasca%</th>\n",
       "      <th>1_LOR</th>\n",
       "      <th>1_LOR_CI_Lower</th>\n",
       "      <th>1_LOR_CI_Upper</th>\n",
       "      <th>1_p_val</th>\n",
       "      <th>2_LOR</th>\n",
       "      <th>2_LOR_CI_Lower</th>\n",
       "      <th>2_LOR_CI_Upper</th>\n",
       "      <th>2_p_val</th>\n",
       "      <th>3_LOR</th>\n",
       "      <th>3_LOR_CI_Lower</th>\n",
       "      <th>3_LOR_CI_Upper</th>\n",
       "      <th>3_p_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Auditory</th>\n",
       "      <td>84.6</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.14</td>\n",
       "      <td>-0.762140</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>3.981016</td>\n",
       "      <td>2.490</td>\n",
       "      <td>5.472</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.653960</td>\n",
       "      <td>2.953</td>\n",
       "      <td>6.355</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Egocentric</th>\n",
       "      <td>80.8</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>0.539129</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>1.688</td>\n",
       "      <td>1.0727</td>\n",
       "      <td>3.734883</td>\n",
       "      <td>2.294</td>\n",
       "      <td>5.176</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.149283</td>\n",
       "      <td>1.786</td>\n",
       "      <td>4.513</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Externally located</th>\n",
       "      <td>73.1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>28.57</td>\n",
       "      <td>-0.385662</td>\n",
       "      <td>-1.517</td>\n",
       "      <td>0.745</td>\n",
       "      <td>1.5118</td>\n",
       "      <td>1.897555</td>\n",
       "      <td>0.814</td>\n",
       "      <td>2.981</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>2.305348</td>\n",
       "      <td>1.147</td>\n",
       "      <td>3.464</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Visual</th>\n",
       "      <td>60.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>-0.614366</td>\n",
       "      <td>-1.659</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.7466</td>\n",
       "      <td>2.121142</td>\n",
       "      <td>0.981</td>\n",
       "      <td>3.262</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>2.817780</td>\n",
       "      <td>1.601</td>\n",
       "      <td>4.035</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olfactory</th>\n",
       "      <td>50.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497838</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>1.482</td>\n",
       "      <td>0.9637</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>1.505</td>\n",
       "      <td>7.284</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>3.942464</td>\n",
       "      <td>1.048</td>\n",
       "      <td>6.837</td>\n",
       "      <td>0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative</th>\n",
       "      <td>46.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.675755</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>1.683</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>4.246029</td>\n",
       "      <td>1.356</td>\n",
       "      <td>7.136</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>3.616745</td>\n",
       "      <td>0.712</td>\n",
       "      <td>6.522</td>\n",
       "      <td>0.0440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tactile</th>\n",
       "      <td>46.2</td>\n",
       "      <td>22.5</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1.050276</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>2.223158</td>\n",
       "      <td>0.892</td>\n",
       "      <td>3.554</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1.222955</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>2.630</td>\n",
       "      <td>0.2651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simple structure</th>\n",
       "      <td>69.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>1.173017</td>\n",
       "      <td>0.149</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>2.446862</td>\n",
       "      <td>1.280</td>\n",
       "      <td>3.614</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.217172</td>\n",
       "      <td>0.077</td>\n",
       "      <td>2.358</td>\n",
       "      <td>0.1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Commanding</th>\n",
       "      <td>26.9</td>\n",
       "      <td>67.5</td>\n",
       "      <td>71.43</td>\n",
       "      <td>-1.667008</td>\n",
       "      <td>-2.732</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>-1.897555</td>\n",
       "      <td>-2.981</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-1.224</td>\n",
       "      <td>0.896</td>\n",
       "      <td>2.2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Commenting</th>\n",
       "      <td>19.2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>-1.167560</td>\n",
       "      <td>-2.288</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>-1.658104</td>\n",
       "      <td>-2.781</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>-0.443220</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internally individualized</th>\n",
       "      <td>100.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2.903940</td>\n",
       "      <td>0.020</td>\n",
       "      <td>5.788</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>6.341870</td>\n",
       "      <td>3.337</td>\n",
       "      <td>9.347</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3.520201</td>\n",
       "      <td>2.114</td>\n",
       "      <td>4.927</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimal personification</th>\n",
       "      <td>61.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21.43</td>\n",
       "      <td>0.056672</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>1.051</td>\n",
       "      <td>2.7331</td>\n",
       "      <td>1.650681</td>\n",
       "      <td>0.590</td>\n",
       "      <td>2.711</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>1.650681</td>\n",
       "      <td>0.590</td>\n",
       "      <td>2.711</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complex personification</th>\n",
       "      <td>38.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.57</td>\n",
       "      <td>-0.056672</td>\n",
       "      <td>-1.051</td>\n",
       "      <td>0.937</td>\n",
       "      <td>2.7331</td>\n",
       "      <td>-1.650681</td>\n",
       "      <td>-2.711</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>-1.650681</td>\n",
       "      <td>-2.711</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agency w/o individuation</th>\n",
       "      <td>38.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>-0.256241</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>0.732</td>\n",
       "      <td>1.8342</td>\n",
       "      <td>1.919593</td>\n",
       "      <td>0.578</td>\n",
       "      <td>3.261</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>2.223158</td>\n",
       "      <td>0.892</td>\n",
       "      <td>3.554</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive on relationships</th>\n",
       "      <td>15.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.124930</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>2.756</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-2.792</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>-2.833213</td>\n",
       "      <td>-4.590</td>\n",
       "      <td>-1.076</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Spiritual%  Psychosis%  Ayahuasca%     1_LOR  \\\n",
       "Code                                                                      \n",
       "Auditory                         84.6        92.5        7.14 -0.762140   \n",
       "Egocentric                       80.8        70.0        7.14  0.539129   \n",
       "Externally located               73.1        80.0       28.57 -0.385662   \n",
       "Visual                           60.0        75.0       14.29 -0.614366   \n",
       "Olfactory                        50.0        37.5        0.00  0.497838   \n",
       "Dissociative                     46.2        30.0        0.00  0.675755   \n",
       "Tactile                          46.2        22.5        7.14  1.050276   \n",
       "Simple structure                 69.2        40.0       14.29  1.173017   \n",
       "Commanding                       26.9        67.5       71.43 -1.667008   \n",
       "Commenting                       19.2        45.0       57.14 -1.167560   \n",
       "Internally individualized       100.0        75.0        7.14  2.903940   \n",
       "Minimal personification          61.5        60.0       21.43  0.056672   \n",
       "Complex personification          38.5        40.0       78.57 -0.056672   \n",
       "Agency w/o individuation         38.5        45.0        7.14 -0.256241   \n",
       "Positive on relationships        15.4         5.0       50.00  1.124930   \n",
       "\n",
       "                           1_LOR_CI_Lower  1_LOR_CI_Upper  1_p_val     2_LOR  \\\n",
       "Code                                                                           \n",
       "Auditory                           -2.254           0.729   0.9497  3.981016   \n",
       "Egocentric                         -0.609           1.688   1.0727  3.734883   \n",
       "Externally located                 -1.517           0.745   1.5118  1.897555   \n",
       "Visual                             -1.659           0.430   0.7466  2.121142   \n",
       "Olfactory                          -0.486           1.482   0.9637  4.394449   \n",
       "Dissociative                       -0.331           1.683   0.5655  4.246029   \n",
       "Tactile                             0.002           2.098   0.1486  2.223158   \n",
       "Simple structure                    0.149           2.197   0.0744  2.446862   \n",
       "Commanding                         -2.732          -0.602   0.0064 -1.897555   \n",
       "Commenting                         -2.288          -0.047   0.1234 -1.658104   \n",
       "Internally individualized           0.020           5.788   0.1454  6.341870   \n",
       "Minimal personification            -0.937           1.051   2.7331  1.650681   \n",
       "Complex personification            -1.051           0.937   2.7331 -1.650681   \n",
       "Agency w/o individuation           -1.245           0.732   1.8342  1.919593   \n",
       "Positive on relationships          -0.506           2.756   0.5294 -1.609438   \n",
       "\n",
       "                           2_LOR_CI_Lower  2_LOR_CI_Upper  2_p_val     3_LOR  \\\n",
       "Code                                                                           \n",
       "Auditory                            2.490           5.472   0.0000  4.653960   \n",
       "Egocentric                          2.294           5.176   0.0000  3.149283   \n",
       "Externally located                  0.814           2.981   0.0018  2.305348   \n",
       "Visual                              0.981           3.262   0.0008  2.817780   \n",
       "Olfactory                           1.505           7.284   0.0086  3.942464   \n",
       "Dissociative                        1.356           7.136   0.0119  3.616745   \n",
       "Tactile                             0.892           3.554   0.0032  1.222955   \n",
       "Simple structure                    1.280           3.614   0.0001  1.217172   \n",
       "Commanding                         -2.981          -0.814   0.0018 -0.164339   \n",
       "Commenting                         -2.781          -0.536   0.0114 -0.443220   \n",
       "Internally individualized           3.337           9.347   0.0001  3.520201   \n",
       "Minimal personification             0.590           2.711   0.0069  1.650681   \n",
       "Complex personification            -2.711          -0.590   0.0069 -1.650681   \n",
       "Agency w/o individuation            0.578           3.261   0.0151  2.223158   \n",
       "Positive on relationships          -2.792          -0.427   0.0230 -2.833213   \n",
       "\n",
       "                           3_LOR_CI_Lower  3_LOR_CI_Upper  3_p_val  \n",
       "Code                                                                \n",
       "Auditory                            2.953           6.355   0.0000  \n",
       "Egocentric                          1.786           4.513   0.0000  \n",
       "Externally located                  1.147           3.464   0.0003  \n",
       "Visual                              1.601           4.035   0.0000  \n",
       "Olfactory                           1.048           6.837   0.0228  \n",
       "Dissociative                        0.712           6.522   0.0440  \n",
       "Tactile                            -0.184           2.630   0.2651  \n",
       "Simple structure                    0.077           2.358   0.1094  \n",
       "Commanding                         -1.224           0.896   2.2838  \n",
       "Commenting                         -1.421           0.534   1.1223  \n",
       "Internally individualized           2.114           4.927   0.0000  \n",
       "Minimal personification             0.590           2.711   0.0069  \n",
       "Complex personification            -2.711          -0.590   0.0069  \n",
       "Agency w/o individuation            0.892           3.554   0.0032  \n",
       "Positive on relationships          -4.590          -1.076   0.0047  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick params for comparison\n",
    "df_1 = df_after #pick df (both, during or after)\n",
    "df_2 = df_after\n",
    "i_1  = 2 # pick comparison (1: spiri-vip, 2: spiri-aya, 3: vip-aya)\n",
    "i_2  = 3\n",
    "i_3 = 1\n",
    "\n",
    "filtered_df1 = filter_df_by_strict_zero_crossing(df_1, f'{i_1}_LOR_CI_Lower', f'{i_2}_LOR_CI_Upper', f'{i_1}_p_val')\n",
    "filtered_df2 = filter_df_by_strict_zero_crossing(df_2, f'{i_2}_LOR_CI_Lower', f'{i_2}_LOR_CI_Upper', f'{i_2}_p_val')\n",
    "filtered_df3 = filter_df_by_strict_zero_crossing(df_2, f'{i_3}_LOR_CI_Lower', f'{i_3}_LOR_CI_Upper', f'{i_3}_p_val')\n",
    "\n",
    "#common elements\n",
    "print('common elements')\n",
    "#print(list(set(list(filtered_df1.index)) & set(list(filtered_df2.index)) & set(list(filtered_df2.index))), '\\n')\n",
    "print(list(set(list(filtered_df1.index)) & set(list(filtered_df2.index))), '\\n')\n",
    "print('comp1: ', list(filtered_df1.index), '\\n')\n",
    "print('comp2: ', list(filtered_df2.index), '\\n')\n",
    "#print('comp3: ', list(filtered_df2.index), '\\n')\n",
    "\n",
    "# the df of interest\n",
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba2016c0-1b3a-482e-8f40-101bb04c9905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Auditory', 'Negative on relationships', 'Olfactory', 'Dissociative', 'Minimal personification', 'Nonverbal', 'Self-stigma', 'Allocentric', 'Complex personification', 'Externally located', 'Internally individualized', 'Agency w/o individuation', 'Positive on relationships', 'Egocentric', 'Visual']\n"
     ]
    }
   ],
   "source": [
    "# used previous cell to compute the common codes during/after for each pair\n",
    "vip_aya = ['Abusive/violent', 'Family narrative', 'Elicit negative emotions', 'Dissociative', 'Multimodal', 'Minimal personification', 'Self-stigma', 'Complex personification', 'Egocentric', 'Recognizable', 'Nonverbal', 'Allocentric', 'Voice knowledge', 'Olfactory', 'Visual imagery', 'Boundary', 'Externally individualized', 'Externally located', 'Supernatural narrative', 'Positive on relationships', 'Visual', 'Agency w/o individuation', 'Positive/helpful', 'Internally individualized', 'Thought-like', 'Elicit positive emotions', 'Auditory', 'Negative on relationships']\n",
    "spiri_aya = ['Archetypal', 'Simple structure', 'Dissociative', 'Minimal personification', 'Self-stigma', 'Complex personification', 'Egocentric', 'Nonverbal', 'Allocentric', 'Tactile', 'Olfactory', 'Idiosyncratic narrative', 'Commenting', 'Externally located', 'Positive on relationships', 'Visual', 'Commanding', 'Agency w/o individuation', 'Volitional', 'Change in frequency', 'Internally individualized', 'Gustatory', 'Auditory', 'Negative on relationships']\n",
    "\n",
    "# now we look at the codes that are sig across groups and contexts\n",
    "vip_spiri_aya = list(set(vip_aya) & set(spiri_aya))\n",
    "print(vip_spiri_aya)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4830d-d232-43b2-b80c-c916d7cc1373",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d75d176e-61b0-47ad-85c8-2f03817933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a forest plot!!\n",
    "#!pip install forestplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07ae71e6-306e-442e-91d0-c06ce709ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestplot as fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5868c46-45e9-46cc-b4d0-4ac0e154a66d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reshape the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba45f1c-38e6-45ea-9cda-712b13a92be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 'Both' dataframe, containing the codes that are not exclusive to during or after the ayahuasca experience\n",
    "df_during['Code'] = df_during.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9d9f19b-27f4-4edc-b6ba-1abc4f44398e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The following 'id_vars' are not present in the DataFrame: ['Code']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10148\\1743768253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# --- 3. Melt the DataFrame to long format ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m df_long = pd.melt(df_both,\n\u001b[0m\u001b[0;32m      7\u001b[0m                   \u001b[0mid_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                   \u001b[0mvalue_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\melt.py\u001b[0m in \u001b[0;36mmelt\u001b[1;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 raise KeyError(\n\u001b[0m\u001b[0;32m     78\u001b[0m                     \u001b[1;34m\"The following 'id_vars' are not present \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                     \u001b[1;34mf\"in the DataFrame: {list(missing)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The following 'id_vars' are not present in the DataFrame: ['Code']\""
     ]
    }
   ],
   "source": [
    "# --- 2. Identify columns to melt ---\n",
    "id_vars = ['Code', 'Spiritual%', 'Psychosis%', 'Ayahuasca%']\n",
    "value_vars = [col for col in df_both.columns if re.match(r'^\\d_LOR(?:_CI_(?:Lower|Upper))?$', col)]\n",
    "\n",
    "# --- 3. Melt the DataFrame to long format ---\n",
    "df_long = pd.melt(df_both,\n",
    "                  id_vars=id_vars,\n",
    "                  value_vars=value_vars,\n",
    "                  var_name='Metric',\n",
    "                  value_name='Value')\n",
    "\n",
    "# --- 4. Extract comparison number and value type from 'Metric' column ---\n",
    "def parse_metric(metric_str):\n",
    "    match = re.match(r'(\\d)_(LOR(?:_CI_(?:Lower|Upper))?)', metric_str)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None, None\n",
    "\n",
    "df_long[['Comparison', 'ValueType']] = df_long['Metric'].apply(lambda x: pd.Series(parse_metric(x)))\n",
    "\n",
    "# --- 5. Pivot the DataFrame back to get 'estimate', 'll', 'hl' columns ---\n",
    "df_reshaped = df_long.pivot_table(index=['Code', 'Comparison'] + id_vars[1:],\n",
    "                                  columns='ValueType',\n",
    "                                  values='Value').reset_index()\n",
    "\n",
    "# --- 6. Rename columns for forestplot compatibility ---\n",
    "df_reshaped = df_reshaped.rename(columns={\n",
    "    'LOR': 'estimate',\n",
    "    'LOR_CI_Lower': 'll',\n",
    "    'LOR_CI_Upper': 'hl'\n",
    "})\n",
    "\n",
    "# Ensure numeric types\n",
    "df_reshaped['estimate'] = pd.to_numeric(df_reshaped['estimate'])\n",
    "df_reshaped['ll'] = pd.to_numeric(df_reshaped['ll'])\n",
    "df_reshaped['hl'] = pd.to_numeric(df_reshaped['hl'])\n",
    "\n",
    "# Convert 'Comparison' to a categorical type and sort for consistent plotting order\n",
    "df_reshaped['Comparison'] = pd.Categorical(df_reshaped['Comparison'], categories=['1', '2', '3'], ordered=True)\n",
    "df_reshaped = df_reshaped.sort_values(by=['Code', 'Comparison']).reset_index(drop=True)\n",
    "\n",
    "# --- 7. Prepare labels for the groupbylabel and display columns ---\n",
    "modellabels_map = {\n",
    "    '1': 'Comparison 1',\n",
    "    '2': 'Comparison 2',\n",
    "    '3': 'Comparison 3'\n",
    "}\n",
    "df_reshaped['Comparison_Label'] = df_reshaped['Comparison'].map(modellabels_map)\n",
    "\n",
    "# Create new columns for display that will be conditionally filled\n",
    "df_reshaped['display_varlabel'] = ''\n",
    "df_reshaped['display_Spiritual%'] = ''\n",
    "df_reshaped['display_Psychosis%'] = ''\n",
    "df_reshaped['display_Ayahuasca%'] = ''\n",
    "\n",
    "# Iterate through the DataFrame to fill the display columns\n",
    "current_code = None\n",
    "for index, row in df_reshaped.iterrows():\n",
    "    if row['Code'] != current_code:\n",
    "        # This is the first row for a new 'Code' group\n",
    "        df_reshaped.loc[index, 'display_Spiritual%'] = row['Spiritual%']\n",
    "        df_reshaped.loc[index, 'display_Psychosis%'] = row['Psychosis%']\n",
    "        df_reshaped.loc[index, 'display_Ayahuasca%'] = row['Ayahuasca%']\n",
    "        current_code = row['Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9702c-93b2-4334-9643-a8b73125b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the during df\n",
    "# --- 2. Identify columns to melt ---\n",
    "id_vars = ['Code', 'Spiritual%', 'Psychosis%', 'Ayahuasca%']\n",
    "value_vars = [col for col in df_during.columns if re.match(r'^\\d_LOR(?:_CI_(?:Lower|Upper))?$', col)]\n",
    "\n",
    "# --- 3. Melt the DataFrame to long format ---\n",
    "df_long = pd.melt(df_during,\n",
    "                  id_vars=id_vars,\n",
    "                  value_vars=value_vars,\n",
    "                  var_name='Metric',\n",
    "                  value_name='Value')\n",
    "\n",
    "# --- 4. Extract comparison number and value type from 'Metric' column ---\n",
    "def parse_metric(metric_str):\n",
    "    match = re.match(r'(\\d)_(LOR(?:_CI_(?:Lower|Upper))?)', metric_str)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None, None\n",
    "\n",
    "df_long[['Comparison', 'ValueType']] = df_long['Metric'].apply(lambda x: pd.Series(parse_metric(x)))\n",
    "\n",
    "# --- 5. Pivot the DataFrame back to get 'estimate', 'll', 'hl' columns ---\n",
    "df_reshaped = df_long.pivot_table(index=['Code', 'Comparison'] + id_vars[1:],\n",
    "                                  columns='ValueType',\n",
    "                                  values='Value').reset_index()\n",
    "\n",
    "# --- 6. Rename columns for forestplot compatibility ---\n",
    "df_reshaped = df_reshaped.rename(columns={\n",
    "    'LOR': 'estimate',\n",
    "    'LOR_CI_Lower': 'll',\n",
    "    'LOR_CI_Upper': 'hl'\n",
    "})\n",
    "\n",
    "# Ensure numeric types\n",
    "df_reshaped['estimate'] = pd.to_numeric(df_reshaped['estimate'])\n",
    "df_reshaped['ll'] = pd.to_numeric(df_reshaped['ll'])\n",
    "df_reshaped['hl'] = pd.to_numeric(df_reshaped['hl'])\n",
    "\n",
    "# Convert 'Comparison' to a categorical type and sort for consistent plotting order\n",
    "df_reshaped['Comparison'] = pd.Categorical(df_reshaped['Comparison'], categories=['1', '2', '3'], ordered=True)\n",
    "df_reshaped = df_reshaped.sort_values(by=['Code', 'Comparison']).reset_index(drop=True)\n",
    "\n",
    "# --- 7. Prepare labels for the groupbylabel and display columns ---\n",
    "modellabels_map = {\n",
    "    '1': 'Comparison 1',\n",
    "    '2': 'Comparison 2',\n",
    "    '3': 'Comparison 3'\n",
    "}\n",
    "df_reshaped['Comparison_Label'] = df_reshaped['Comparison'].map(modellabels_map)\n",
    "\n",
    "# Create new columns for display that will be conditionally filled\n",
    "df_reshaped['display_varlabel'] = ''\n",
    "df_reshaped['display_Spiritual%'] = ''\n",
    "df_reshaped['display_Psychosis%'] = ''\n",
    "df_reshaped['display_Ayahuasca%'] = ''\n",
    "\n",
    "# Iterate through the DataFrame to fill the display columns\n",
    "current_code = None\n",
    "for index, row in df_reshaped.iterrows():\n",
    "    if row['Code'] != current_code:\n",
    "        # This is the first row for a new 'Code' group\n",
    "        df_reshaped.loc[index, 'display_Spiritual%'] = row['Spiritual%']\n",
    "        df_reshaped.loc[index, 'display_Psychosis%'] = row['Psychosis%']\n",
    "        df_reshaped.loc[index, 'display_Ayahuasca%'] = row['Ayahuasca%']\n",
    "        current_code = row['Code']\n",
    "df_reshaped_during = df_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffede7-ddba-4786-b07f-3c5c0ef8ebb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plots!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842dd06-3606-4216-89a9-9257685d36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Plot the reshaped DataFrame using fp.forestplot with custom display columns ---\n",
    "fp.forestplot(\n",
    "    dataframe=df_reshaped,\n",
    "    estimate=\"estimate\",\n",
    "    ll=\"ll\",\n",
    "    hl=\"hl\",\n",
    "    varlabel=\"Comparison_Label\", # Use the new column with conditional empty strings\n",
    "    capitalize= \"capitalize\",\n",
    "    color_alt_rows=True,\n",
    "    table=True, # Display a table on the right side\n",
    "    rightannote=[\"display_Spiritual%\", \"display_Psychosis%\", \"display_Ayahuasca%\"],\n",
    "    right_annoteheaders=[\"Spiritual%\", \"Psychosis%\", \"Ayahuasca%\"],\n",
    "    xlabel=\"Log Odds Ratio (95% CI)\",\n",
    "    xticks=[-8, -4, 0, 4, 8],\n",
    "    groupvar= 'Code',\n",
    "    group_order = list(df_reshaped['Code'].unique()),\n",
    "    # Adjusting offset to 0 for better alignment\n",
    "    **{\n",
    "        \"markersize\": 30,\n",
    "        \"offset\": 0.0, # Setting offset to 0.0 to minimize misalignment\n",
    "        \"xlinestyle\": (0, (10, 5)),\n",
    "        \"xlinecolor\": \".8\",\n",
    "        \"figsize\": (10, 8),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0ea50-6831-4b1e-bc5b-cc9a3265b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Plot the reshaped DataFrame using fp.forestplot with custom display columns ---\n",
    "fp.forestplot(\n",
    "    dataframe=df_reshaped_during,\n",
    "    estimate=\"estimate\",\n",
    "    ll=\"ll\",\n",
    "    hl=\"hl\",\n",
    "    varlabel=\"Comparison_Label\", # Use the new column with conditional empty strings\n",
    "    capitalize= \"capitalize\",\n",
    "    color_alt_rows=True,\n",
    "    table=True, # Display a table on the right side\n",
    "    rightannote=[\"display_Spiritual%\", \"display_Psychosis%\", \"display_Ayahuasca%\"],\n",
    "    right_annoteheaders=[\"Spiritual%\", \"Psychosis%\", \"Ayahuasca%\"],\n",
    "    xlabel=\"Log Odds Ratio (95% CI)\",\n",
    "    xticks=[-8, -4, 0, 4, 8],\n",
    "    groupvar= 'Code',\n",
    "    group_order = list(df_reshaped_during['Code'].unique()),\n",
    "    # Adjusting offset to 0 for better alignment\n",
    "        # Adjusting offset to 0 for better alignment\n",
    "    **{\n",
    "        \"markersize\": 30,\n",
    "        \"offset\": 0.0, # Setting offset to 0.0 to minimize misalignment\n",
    "        \"xlinestyle\": (0, (10, 5)),\n",
    "        \"xlinecolor\": \".8\",\n",
    "        \"figsize\": (8,40),\n",
    "    },\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ccbe4-e10a-47d3-b111-1983061187f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stat test during/after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7908c31e-db11-4862-9d61-2b8e14e3e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sc\n",
    "from scipy.stats import fisher_exact, mannwhitneyu\n",
    "from scipy.stats.contingency import chi2_contingency\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab403648-b9bd-4d29-ac1f-c1f439fe421d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>B1</th>\n",
       "      <th>...</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A1   A2   A3   A4   A5   A6   A7   A8   A9   B1  ...   T7   T8   T9  T10  \\\n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  0.0  1.0  1.0   \n",
       "4   1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
       "5   1.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  0.0  1.0   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  0.0  1.0  0.0   \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  1.0  1.0   \n",
       "8   1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "11  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  1.0  1.0  0.0   \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  1.0  0.0  0.0   \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  1.0  0.0  0.0   \n",
       "\n",
       "    T11  T12  T13  T14 X1 X2  \n",
       "1   0.0  1.0  0.0  1.0  0  1  \n",
       "2   0.0  1.0  0.0  0.0  1  1  \n",
       "4   1.0  1.0  0.0  0.0  1  1  \n",
       "5   0.0  1.0  0.0  0.0  1  1  \n",
       "6   1.0  1.0  0.0  0.0  0  1  \n",
       "7   0.0  1.0  0.0  0.0  0  1  \n",
       "8   1.0  1.0  0.0  0.0  0  0  \n",
       "9   0.0  1.0  0.0  0.0  0  1  \n",
       "10  0.0  1.0  0.0  0.0  0  1  \n",
       "11  0.0  1.0  0.0  0.0  1  1  \n",
       "12  1.0  1.0  0.0  0.0  1  1  \n",
       "13  0.0  1.0  0.0  0.0  1  1  \n",
       "14  1.0  1.0  0.0  0.0  0  0  \n",
       "\n",
       "[13 rows x 206 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_final.drop(['Question_text', 'Percentages', 'Total'], axis = 1).transpose()\n",
    "df = df.drop(3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41ec394e-d8c7-455d-98f2-12dc1b894b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_custom_latex_table(df, caption=\"Descriptive statistics of the voice phenomenology during and after ayahuasca.\"):\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame into a custom LaTeX table format,\n",
    "    mimicking the provided structure with grouped rows and specific columns.\n",
    "    NaN values in data columns will be represented by a hyphen '-'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame. Expected columns:\n",
    "                           'Question 1', 'Question 2', 'test_name', 'statistic',\n",
    "                           'P-value', 'Pcorr', 'dof'.\n",
    "        caption (str): The caption for the LaTeX table.\n",
    "        wrap_table_pos (str): Position for wraptable ('l' for left, 'r' for right).\n",
    "        wrap_table_width (str): Width for wraptable (e.g., '7cm', '0.5\\\\textwidth').\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the LaTeX table code.\n",
    "    \"\"\"\n",
    "    latex_code = []\n",
    "\n",
    "    # Start of the wraptable environment\n",
    "    latex_code.append(f\"\\\\begin{{table}}\")\n",
    "    latex_code.append(\"    \\\\centering\")\n",
    "    latex_code.append(\"    \\\\begingroup\")\n",
    "    latex_code.append(\"    \\\\footnotesize\")\n",
    "    latex_code.append(\"    \\\\setlength{\\\\extrarowheight}{1pt}\")\n",
    "\n",
    "    # Define columns: 1st column ragged right, rest centered\n",
    "    latex_code.append(\"    \\\\begin{tabular}{p{4cm} cccc}\") # 1 custom p-column + 5 c-columns (6 total)\n",
    "\n",
    "    latex_code.append(\"    \\\\toprule\")\n",
    "    # Header row\n",
    "    latex_code.append(\"    \\\\textbf{Item} & \\\\textbf{Test name} & \\\\textbf{stat} & \\\\textbf{$p-value$} & \\\\textbf{$p_{corr}$} \\\\\\\\\")\n",
    "    latex_code.append(\"    \\\\midrule\")\n",
    "\n",
    "    previous_q1 = None\n",
    "    for idx, row in df.iterrows():\n",
    "        current_q1 = row['Question 1']\n",
    "        current_q2 = row['Question 2']\n",
    "\n",
    "        # Format data fields, handling NaN values with '-'\n",
    "        test_name_val = '-' if pd.isnull(row['test_name']) else str(row['test_name'])\n",
    "        statistic_val = '-' if pd.isnull(row['statistic']) else f\"{row['statistic']:.2f}\"\n",
    "        p_value_val = '-' if pd.isnull(row['P-value']) else f\"{row['P-value']:.3f}\"\n",
    "        pcorr_val = '-' if pd.isnull(row['Pcorr']) else f\"{row['Pcorr']:.3f}\" \n",
    "\n",
    "        # If Question 1 changes, print the bolded Question 1 header (and add spacing for previous group)\n",
    "        if current_q1 != previous_q1:\n",
    "            if previous_q1 is not None: # Add spacing after a group if it's not the very first group\n",
    "                latex_code.append(\"    [1ex]\")\n",
    "\n",
    "            # If Q2 is empty/NaN for the current Q1, it means Q1 itself is the item with data (e.g., \"Audible\")\n",
    "            # If Q2 has values, Q1 is just a category header (e.g., \"Location\")\n",
    "            if pd.isnull(current_q2) or current_q2 == '':\n",
    "                # This row is for a main item that also has data\n",
    "                item_display = f\"\\\\textbf{{{current_q1}}}\"\n",
    "            else:\n",
    "                # This row is for a sub-item group, so first print Q1 as a category header\n",
    "                item_display = f\"\\\\hspace{{3mm}} {current_q2}\" # Now set item_display for the sub-item\n",
    "            previous_q1 = current_q1 # Update previous_q1 after processing the category header\n",
    "        else:\n",
    "            # If Question 1 is the same as the previous row, it's a sub-item\n",
    "            item_display = f\"\\\\hspace{{3mm}} {current_q2}\"\n",
    "\n",
    "\n",
    "        latex_code.append(\n",
    "            f\"    {item_display} & \"\n",
    "            f\"{test_name_val} & \"\n",
    "            f\"{statistic_val} & \"\n",
    "            f\"{p_value_val} & \"\n",
    "            f\"{pcorr_val}  \\\\\\\\\"\n",
    "        )\n",
    "    latex_code.append(\"    [1ex]\") # Add a final [1ex] after the last row\n",
    "\n",
    "    latex_code.append(\"    \\\\bottomrule\")\n",
    "    latex_code.append(\"    \\\\end{tabular}\")\n",
    "    latex_code.append(f\"    \\\\caption{{\\\\textbf{{{caption}}}}}\")\n",
    "    latex_code.append(\"    \\\\endgroup\")\n",
    "    latex_code.append(\"\\\\end{table}\")\n",
    "\n",
    "    return \"\\n\".join(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2b4c91e-1353-4aab-ad0e-15a4a16fe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcnemar_test(contingency_table):\n",
    "    \"\"\"\n",
    "    Performs McNemar's test on two paired categorical columns (before and after responses). Works on dichotomous data\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset.\n",
    "    col_before (str or int): The column name or index representing the \"before\" responses.\n",
    "    col_after (str or int): The column name or index representing the \"after\" responses.\n",
    "\n",
    "    Returns:\n",
    "    float: The p-value from McNemar's test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy array for McNemar test\n",
    "    table = contingency_table.values\n",
    "\n",
    "    # Run McNemar's test\n",
    "    result = mcnemar(table, exact=True) #what about third param\n",
    "\n",
    "    return result.pvalue, result.statistic\n",
    "\n",
    "\n",
    "\n",
    "def stat_test(df, col1, col2, paired=True):\n",
    "    \"\"\"\n",
    "    Performs appropriate statistical test (McNemar's or Fisher's exact).\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame.\n",
    "        col1: Name of the first column (e.g., 'Before', 'ConditionA').\n",
    "        col2: Name of the second column (e.g., 'After', 'ConditionB').\n",
    "        paired: True if the data is paired (attempts McNemar's), False otherwise (attempts Fisher's).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the test results:\n",
    "            'test_name': Name of the test performed.\n",
    "            'p_value': P-value of the test.\n",
    "            'statistic': Test statistic.\n",
    "            'warning': A warning message if applicable.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'test_name': None,\n",
    "        'p_value': None,\n",
    "        'statistic': None,\n",
    "        'warning': None\n",
    "    }\n",
    "\n",
    "    # Basic input validation\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        results['warning'] = \"One or both specified columns not found in the DataFrame.\"\n",
    "        return results\n",
    "\n",
    "    # Create the 2x2 contingency table directly from the 0/1 columns\n",
    "    # pd.crosstab will correctly handle 0s and 1s as categories and create the 2x2 table.\n",
    "    contingency_table = pd.crosstab(df[col1], df[col2])\n",
    "\n",
    "    # Ensure the table is 2x2 and fill missing combinations with 0\n",
    "    # This step is still important because if one combination doesn't exist, crosstab won't include it.\n",
    "    # We explicitly define the index and columns for a 2x2 table of 0s and 1s.\n",
    "    required_index = [0, 1]\n",
    "    required_columns = [0, 1]\n",
    "    \n",
    "    # Reindex the contingency table to ensure it's a 2x2 matrix with 0s for missing cells\n",
    "    ct_array = contingency_table.reindex(index=required_index, columns=required_columns, fill_value=0).values\n",
    "\n",
    "    if paired:\n",
    "        # --- Attempt McNemar's Test ---\n",
    "        results['test_name'] = \"McNemar's Test\"\n",
    "        b = ct_array[0, 1]  # Discordant: col1=0, col2=1\n",
    "        c = ct_array[1, 0]  # Discordant: col1=1, col2=0\n",
    "\n",
    "        # McNemar's test is based on discordant pairs. If there are no discordant pairs,\n",
    "        # the p-value is typically 1, meaning no difference, or the test might be undefined.\n",
    "        # It's common to fall back to Fisher's exact if discordant cells are zero.\n",
    "        if b == 0 and c == 0:\n",
    "            results['warning'] = \"McNemar's test has zero discordant cells. Performing Fisher's Exact Test instead.\"\n",
    "            try:\n",
    "                statistic, p_value = fisher_exact(ct_array)\n",
    "                results['test_name'] = \"Fisher's Exact Test (Fallback)\"\n",
    "                results['statistic'] = statistic\n",
    "                results['p_value'] = p_value\n",
    "            except Exception as e:\n",
    "                results['warning'] = f\"Fisher's Exact Test (fallback) failed: {e}\"\n",
    "        else:\n",
    "            try:\n",
    "                mcnemar_result = mcnemar(ct_array, exact=True)\n",
    "                results['statistic'] = mcnemar_result.statistic\n",
    "                results['p_value'] = mcnemar_result.pvalue\n",
    "            except Exception as e:\n",
    "                results['warning'] = f\"McNemar's test failed: {e}. Attempting Fisher's Exact Test as fallback.\"\n",
    "                try:\n",
    "                    statistic, p_value = fisher_exact(ct_array)\n",
    "                    results['test_name'] = \"Fisher's Exact Test (Fallback)\"\n",
    "                    results['statistic'] = statistic\n",
    "                    results['p_value'] = p_value\n",
    "                except Exception as fe:\n",
    "                    results['warning'] += f\" Fisher's Exact Test (fallback) also failed: {fe}\"\n",
    "    else:\n",
    "        # --- Attempt Fisher's Exact Test directly (for unpaired 2x2) ---\n",
    "        results['test_name'] = \"Fisher's Exact Test\"\n",
    "        try:\n",
    "            statistic, p_value = fisher_exact(ct_array)\n",
    "            results['statistic'] = statistic\n",
    "            results['p_value'] = p_value\n",
    "        except Exception as e:\n",
    "            results['warning'] = f\"Fisher's Exact Test failed: {e}. Check your data format or if a 2x2 table can be formed.\"\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e1b081d-cfe6-489c-bd50-5dc90cc47719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalue      0.0625\n",
      "statistic   0.0\n"
     ]
    }
   ],
   "source": [
    "contingency_table = pd.crosstab(df['O2'], df['G2'])\n",
    "print(mcnemar(contingency_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b13ab34-617a-485c-a6a8-e5753c73c3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['X1'])/14\n",
    "#sum(df['X2'])/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2ece447-1375-466a-a149-3ae370092536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if ability to control is sig\n",
    "results = stat_test(df, 'X1',  'X2', paired= True)\n",
    "results['p_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef407053-dae9-4eec-88c2-a07e6952a871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tactile hallucination'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[col1].Question_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31e5e878-305b-4172-9ac2-fa34db9571dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "    \\begin{tabular}{p{4cm} cccc}\n",
      "    \\toprule\n",
      "    \\textbf{Item} & \\textbf{Test name} & \\textbf{stat} & \\textbf{$p-value$} & \\textbf{$p_{corr}$} \\\\\n",
      "    \\midrule\n",
      "    \\hspace{3mm} ‘Auditory’ voices & McNemar's Test & 0.00 & 0.062 & 0.062  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Olfactory hallucination & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Gustatory hallucination & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Dissociative experiences & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Bodily states & McNemar's Test & 1.00 & 0.375 & 0.375  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} ‘Thought-like’ voices & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Telepathic voices & McNemar's Test & 0.00 & 0.250 & 0.250  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Multisensory voices & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Non-verbal sounds & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Visual hallucination & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Visual imagery & McNemar's Test & 0.00 & 0.062 & 0.062  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Felt presences & McNemar's Test & 0.00 & 0.250 & 0.250  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Tactile hallucination & Fisher's Exact Test (Fallback) & inf & 0.077 & 0.077  \\\\\n",
      "    [1ex]\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Descriptive statistics of the voice phenomenology during and after ayahuasca.}}\n",
      "    \\endgroup\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "#for the modatlities table\n",
    "col2_list = list(df_final['J1':'J9'].index)\n",
    "results_list = []\n",
    "\n",
    "for i, col1 in enumerate(list(df_final['B1':'B9'].index)):\n",
    "    col2 = col2_list[i]\n",
    "\n",
    "    #perform the test\n",
    "    results = stat_test(df, col1,  col2, paired= True)\n",
    "    p = results['p_value']\n",
    "\n",
    "    #compute the corrected p value\n",
    "    pcorr = p\n",
    "    \n",
    "    if pcorr is not None and pcorr < 0.05:\n",
    "        sig = 'SIGNIFICANT'\n",
    "    else:\n",
    "        sig = 'not sig'\n",
    "        \n",
    "\n",
    "    # Append the results as a dictionary to the list\n",
    "    results_list.append({\n",
    "        'Question 1': df_final.loc[col1].Question_text ,  # Store column names, not indices\n",
    "        'Question 2': df_final.loc[col2].Question_text ,\n",
    "        'P-value': p,\n",
    "        'Pcorr': pcorr,\n",
    "        'Significance': sig,\n",
    "        **results #unpack the dictionary\n",
    "    })\n",
    "\n",
    "# Create the DataFrame *outside* the loop\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.head()\n",
    "\n",
    "print(df_to_custom_latex_table(results_df)) #printing latex table for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "813a7e9e-d91e-4511-89b9-62cbaa67f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "    \\begin{tabular}{p{4cm} cccc}\n",
      "    \\toprule\n",
      "    \\textbf{Item} & \\textbf{Test name} & \\textbf{stat} & \\textbf{$p-value$} & \\textbf{$p_{corr}$} \\\\\n",
      "    \\midrule\n",
      "    \\hspace{3mm} Internally located voices? & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Externally located voices? & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} ‘Boundary’ voices & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} ‘Egocentric voices & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} ‘Allocentric’ voices & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Predominant & specific location reported? & McNemar's Test & 2.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Recurring voices & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Simple structure & McNemar's Test & 0.00 & 0.125 & 0.125  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Direct address & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Commenting voices? & McNemar's Test & 2.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Conversational voices? & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Commanding voices? & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Follow commands? & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Abusive/violent/threatening voices? & McNemar's Test & 2.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Positive/helpful voices? & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Companionship from voice & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Contains a message & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Recognisable voices & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Voice knowledge & Fisher's Exact Test (Fallback) & inf & 0.077 & 0.077  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Change in frequency of voice hearing experience over time & McNemar's Test & 1.00 & 0.219 & 0.219  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Minimal personification & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Complex personification & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Archetypal features & McNemar's Test & 0.00 & 0.125 & 0.125  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Voice character change over time & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Voices elicit positive emotions & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Voices elicit negative emotions & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Anxiety-associated & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Depression-associated & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Fear-associated & McNemar's Test & 2.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Paranoia-associated & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Absent agency & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Agency without individuation & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Internally individualised agency & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Externally individualised agency & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Descriptive statistics of the voice phenomenology during and after ayahuasca.}}\n",
      "    \\endgroup\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "col2_list = list(df_final['C1':'M4'].index)\n",
    "results_list = []\n",
    "\n",
    "for i, col1 in enumerate(list(df_final['K1':'M4'].index)):\n",
    "    col2 = col2_list[i]\n",
    "\n",
    "    #perform the test\n",
    "    results = stat_test(df, col1,  col2, paired= True)\n",
    "    p = results['p_value']\n",
    "\n",
    "    #compute the corrected p value\n",
    "    pcorr = p\n",
    "    \n",
    "    if pcorr is not None and pcorr < 0.05:\n",
    "        sig = 'SIGNIFICANT'\n",
    "    else:\n",
    "        sig = 'not sig'\n",
    "        \n",
    "\n",
    "    # Append the results as a dictionary to the list\n",
    "    results_list.append({\n",
    "        'Question 1': df_final.loc[col1].Question_text ,  # Store column names, not indices\n",
    "        'Question 2': df_final.loc[col2].Question_text ,\n",
    "        'P-value': p,\n",
    "        'Pcorr': pcorr,\n",
    "        'Significance': sig,\n",
    "        **results #unpack the dictionary\n",
    "    })\n",
    "\n",
    "# Create the DataFrame *outside* the loop\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df\n",
    "print(df_to_custom_latex_table(results_df)) #printing latex table for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb6f2734-6116-4567-be56-577bccd682ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "    \\centering\n",
      "    \\begingroup\n",
      "    \\footnotesize\n",
      "    \\setlength{\\extrarowheight}{1pt}\n",
      "    \\begin{tabular}{p{4cm} cccc}\n",
      "    \\toprule\n",
      "    \\textbf{Item} & \\textbf{Test name} & \\textbf{stat} & \\textbf{$p-value$} & \\textbf{$p_{corr}$} \\\\\n",
      "    \\midrule\n",
      "    \\hspace{3mm} Presence of voices & McNemar's Test & 3.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Tactile contacts & McNemar's Test & 0.00 & 0.500 & 0.500  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} State of mind & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Calm, relaxed state & McNemar's Test & 0.00 & 0.250 & 0.250  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Anxious, stressed or alert state & McNemar's Test & 0.00 & 0.250 & 0.250  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Depressed state & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Events that recently happened & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Intensity of voices & McNemar's Test & 2.00 & 0.688 & 0.688  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Content & McNemar's Test & 0.00 & 0.250 & 0.250  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Emotional tone & McNemar's Test & 1.00 & 0.625 & 0.625  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Location & McNemar's Test & 2.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Presence or actions of facilitators & McNemar's Test & 0.00 & 0.001 & 0.001  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Presence or actions of surrounding people & McNemar's Test & 1.00 & 0.375 & 0.375  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Absence of surrounding people & McNemar's Test & 0.00 & 0.062 & 0.062  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Music & McNemar's Test & 0.00 & 0.001 & 0.001  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Odours & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Does the first voice hearing experience was volitional? & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Presence/absence of the voice & McNemar's Test & 0.00 & 0.062 & 0.062  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Frequency & McNemar's Test & 0.00 & 0.125 & 0.125  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Intensity & McNemar's Test & 0.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Content & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Emotional tone & McNemar's Test & 2.00 & 0.289 & 0.289  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Growing sense of control over time & McNemar's Test & 1.00 & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Decreasing sense of control over time & Fisher's Exact Test (Fallback) & - & 1.000 & 1.000  \\\\\n",
      "    [1ex]\n",
      "    \\hspace{3mm} Development of cultivation of voices technics over time & McNemar's Test & 0.00 & 0.062 & 0.062  \\\\\n",
      "    [1ex]\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Descriptive statistics of the voice phenomenology during and after ayahuasca.}}\n",
      "    \\endgroup\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "col2_list = list(df_final['F1':'G9'].index)\n",
    "results_list = []\n",
    "\n",
    "for i, col1 in enumerate(list(df_final['N1':'O9'].index)):\n",
    "    col2 = col2_list[i]\n",
    "\n",
    "    #perform the test\n",
    "    results = stat_test(df, col1,  col2, paired= True)\n",
    "    p = results['p_value']\n",
    "\n",
    "    #compute the corrected p value\n",
    "    pcorr = p\n",
    "    \n",
    "    if pcorr is not None and pcorr < 0.05:\n",
    "        sig = 'SIGNIFICANT'\n",
    "    else:\n",
    "        sig = 'not sig'\n",
    "        \n",
    "\n",
    "    # Append the results as a dictionary to the list\n",
    "    results_list.append({\n",
    "        'Question 1': df_final.loc[col1].Question_text ,  # Store column names, not indices\n",
    "        'Question 2': df_final.loc[col2].Question_text ,\n",
    "        'P-value': p,\n",
    "        'Pcorr': pcorr,\n",
    "        'Significance': sig,\n",
    "        **results #unpack the dictionary\n",
    "    })\n",
    "\n",
    "# Create the DataFrame *outside* the loop\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df\n",
    "print(df_to_custom_latex_table(results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e1ce5-ab18-44fe-abf5-81b7004de242",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Add inter-rater k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1bf5d-72ee-43db-a706-615d2fabb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import barbara's codings\n",
    "docx_dir2 = \"C:/Users/anaho/Desktop/research/Ayahuasca/DATA/codagesBarb\"\n",
    "\n",
    "# 1. Get all DOCX files in the directory, sorted alphanumerically\n",
    "docx_files = sorted([f for f in os.listdir(docx_dir2) if f.endswith(\".docx\")])\n",
    "\n",
    "all_data = {}  # Store data from all files, keyed by participant ID\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#make question dict\n",
    "question_dict = {}\n",
    "\n",
    " # 2. Iterate through each DOCX file\n",
    "for docx_file in docx_files:\n",
    "    docx_path = os.path.join(docx_dir2, docx_file)\n",
    "    try:\n",
    "        document = docx.Document(docx_path)\n",
    "        num_tables = len(document.tables)\n",
    "        print(f\"Processing {num_tables} tables in {docx_file}\")\n",
    "        if num_tables == 0:\n",
    "            print(f\"No tables found in {docx_file}, skipping.\")\n",
    "            continue\n",
    "\n",
    "    except docx.exceptions.EmptyPackageError as e:\n",
    "        print(f\"Error reading {docx_file}: {e}.  Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    # Extract participant name from filename (remove \".docx\")\n",
    "    participant_name = os.path.splitext(docx_file)[0]  # e.g., \"codage_01\"\n",
    "\n",
    "    # Extract participant ID (e.g., \"01\")\n",
    "    participant_id = participant_name[-2:]\n",
    "    \n",
    "    all_table_data= [] #instantiate list\n",
    "\n",
    "    # 3. Iterate through each table in the document\n",
    "    for table_index, table in enumerate(document.tables):\n",
    "        print(f\"  Processing table {table_index + 1} of {num_tables} in {docx_file}\")\n",
    "        table_data = {}  # Store data for the current table, keyed by question code\n",
    "        # 4. Extract data from the table, handling LetterNumber rows\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]  # Remove extra whitespace\n",
    "            if cells:\n",
    "                first_cell_value = cells[0]\n",
    "                if re.match(r\"^[A-Za-z]\\d+$\", first_cell_value) or re.match(r\"^[A-Za-z]\\d[A-Za-z]+$\", first_cell_value) or re.match(r\"^[A-Za-z]\\d\\d[A-Za-z]+$\", first_cell_value):\n",
    "                    question_code = first_cell_value\n",
    "                    question_text = cells[1]  # Extract the question text\n",
    "                    yes_val = cells[2].lower()\n",
    "                    no_val = cells[3].lower()\n",
    "                    note = cells[4]\n",
    "                    response = None # Initialize response\n",
    "\n",
    "                    if \"x\" in yes_val:\n",
    "                        response = 1.0\n",
    "                    elif \"x\" in no_val:\n",
    "                        response = 0.0\n",
    "                    else:\n",
    "                         response = 0.0 #replacing by 0 the missing x\n",
    "\n",
    "                    table_data[question_code] = {\"response\": response, \"note\": note}\n",
    "                    question_dict[question_code] = question_text #store the question and text.\n",
    "\n",
    "        \n",
    "            '''    else:\n",
    "                    print(\n",
    "                        f\"    Skipping row in {docx_file}, table {table_index + 1} because the first cell '{first_cell_value}' does not match 'A1' format.\")\n",
    "            else:\n",
    "                print(f\"    Skipping empty row in {docx_file}, table {table_index + 1}.\")'''\n",
    "\n",
    "        # 5. Store data for this participant and question\n",
    "        all_table_data.append(table_data)  # Use participant_id\n",
    "\n",
    "    # Flatten the list of dictionaries into a single dictionary for the participant\n",
    "    all_table_data_flat = {}\n",
    "    for item in all_table_data:\n",
    "        all_table_data_flat.update(item)\n",
    "\n",
    "    all_data[participant_id] = all_table_data_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2420ac-4d80-41b6-abe8-bcc7dd28cf95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # 6. Prepare data for Excel, transposing and combining\n",
    "# Prepare data for Excel sheet\n",
    "excel_data = []\n",
    "# Get all unique questions\n",
    "unique_questions = set()\n",
    "for data in all_data.values():\n",
    "    unique_questions.update(data.keys())\n",
    "unique_questions = sorted(list(unique_questions))\n",
    "\n",
    "# Create the header row\n",
    "header_row = [\"Participants\"]  # Changed Header\n",
    "for question in unique_questions:\n",
    "    header_row.extend([f\"{question}\"])\n",
    "excel_data.append(header_row)\n",
    "\n",
    "# Create participant rows\n",
    "for participant_id in participant_ids:\n",
    "    participant_data = all_data[participant_id]\n",
    "    participant_row = [participant_id] # Start with participant ID\n",
    "\n",
    "    for question in unique_questions:\n",
    "        if question in participant_data:\n",
    "            # If the question exists, append the response\n",
    "            # participant_row.extend([participant_data[question]['response'], participant_data[question]['note']]) # if including notes\n",
    "            participant_row.extend([participant_data[question]['response']]) # only codes, no notes\n",
    "        else:\n",
    "            # If the question is not in participant_data, append NaN\n",
    "            # You might print here for debugging if a question is unexpectedly missing\n",
    "            # print(f\"Question '{question}' not found for participant '{participant_id}', appending NaN.\")\n",
    "            participant_row.append(np.nan) # Append NaN for missing data\n",
    "    excel_data.append(participant_row)\n",
    "\n",
    "# 7. Put everything in a dataframe!!\n",
    "df = pd.DataFrame(excel_data, columns = header_row).drop([0])\n",
    "df_final2 = df.transpose().drop(['Participants'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada33bb-df83-4726-89a5-bfe210295e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.loc['N1':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd356f-bb40-4e36-9979-d1a08409fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "df1 = df_final\n",
    "df2 = df_final2\n",
    "\n",
    "# 1. Find common rows (questions)\n",
    "common_rows = df1.index.intersection(df2.index)\n",
    "print(f\"\\nCommon rows: {list(common_rows)}\")\n",
    "\n",
    "# Filter dataframes to include only common rows\n",
    "df1_common_rows = df1.loc[common_rows]\n",
    "df2_common_rows = df2.loc[common_rows]\n",
    "\n",
    "# 2. Find common participants (columns)\n",
    "common_participants = df1.columns.intersection(df2.columns)\n",
    "print(f\"Common participants: {list(common_participants)}\")\n",
    "\n",
    "# Ensure there are enough common participants to choose from\n",
    "if len(common_participants) < 5:\n",
    "    print(f\"\\nWarning: Not enough common participants ({len(common_participants)}) to select 5 random ones. Selecting all common participants.\")\n",
    "    participants_to_compare = common_participants\n",
    "else:\n",
    "    # 3. Select 5 random common participants\n",
    "    participants_to_compare = np.random.choice(common_participants, 5, replace=False)\n",
    "\n",
    "print(f\"\\nSelected participants for comparison: {list(participants_to_compare)}\")\n",
    "\n",
    "# 4. Compute Cohen's Kappa and perc agreement for each selected participant\n",
    "kappa_results = {}\n",
    "perc_agreement_results = {}\n",
    "\n",
    "for participant in participants_to_compare:\n",
    "    if participant in df1_common_rows.columns and participant in df2_common_rows.columns:\n",
    "        # Extract the series for the current participant from both dataframes, using only common rows\n",
    "        ratings_df1 = df1_common_rows[participant]\n",
    "        ratings_df2 = df2_common_rows[participant]\n",
    "\n",
    "        # Attempt to convert to numeric type. 'coerce' will turn unconvertible values into NaN\n",
    "        ratings_df1_numeric = pd.to_numeric(ratings_df1, errors='coerce')\n",
    "        ratings_df2_numeric = pd.to_numeric(ratings_df2, errors='coerce')\n",
    "\n",
    "        # Combine the two series into a temporary DataFrame to easily drop NaNs row-wise\n",
    "        combined_ratings = pd.DataFrame({'df1': ratings_df1_numeric, 'df2': ratings_df2_numeric})\n",
    "\n",
    "        # Drop rows where either rating is NaN (this handles original NaNs AND coerced NaNs)\n",
    "        combined_ratings_cleaned = combined_ratings.dropna()\n",
    "\n",
    "        # Extract the cleaned series for kappa calculation\n",
    "        y1_cleaned = combined_ratings_cleaned['df1']\n",
    "        y2_cleaned = combined_ratings_cleaned['df2']\n",
    "\n",
    "        # Check if there's enough data left after dropping NaNs\n",
    "        if len(y1_cleaned) == 0:\n",
    "            print(f\"Skipping Participant {participant}: No common non-NaN ratings found after filtering and numeric conversion.\")\n",
    "            continue\n",
    "        elif len(y1_cleaned) < 2: # At least two data points needed for kappa\n",
    "            print(f\"Skipping Participant {participant}: Less than 2 common non-NaN ratings found after filtering and numeric conversion.\")\n",
    "            continue\n",
    "\n",
    "        # Crucial check: Ensure there are at least two *distinct* classes in the data after cleaning\n",
    "        # for cohen_kappa_score to make sense. If all values are the same, it implies zero variance\n",
    "        # which can cause issues or result in an undefined kappa.\n",
    "        if len(y1_cleaned.unique()) < 2 or len(y2_cleaned.unique()) < 2:\n",
    "            print(f\"Skipping Participant {participant}: One or both datasets have less than 2 unique rating categories after cleaning. Kappa is undefined.\")\n",
    "            continue\n",
    "        \n",
    "        # --- Compute Percentage Agreement ---\n",
    "        agreements = (y1_cleaned == y2_cleaned).sum()\n",
    "        total_observations = len(y1_cleaned)\n",
    "        percentage_agreement = (agreements / total_observations) * 100\n",
    "        perc_agreement_results[participant] = percentage_agreement\n",
    "\n",
    "\n",
    "        # Compute Cohen's Kappa\n",
    "        try:\n",
    "            kappa = cohen_kappa_score(y1_cleaned, y2_cleaned)\n",
    "            kappa_results[participant] = kappa\n",
    "        except ValueError as e:\n",
    "            print(f\"Error computing kappa for Participant {participant}: {e}\")\n",
    "            print(f\"Problematic data for {participant}:\")\n",
    "            print(f\"  y1_cleaned: {y1_cleaned.tolist()}\")\n",
    "            print(f\"  y2_cleaned: {y2_cleaned.tolist()}\")\n",
    "            # This is where you might see the \"unknown and binary targets\" error again if\n",
    "            # it's related to something subtle like float representation of integers, etc.\n",
    "    else:\n",
    "        print(f\"Participant {participant} not found in both filtered dataframes (this should not happen if selected from common_participants).\")\n",
    "\n",
    "if not kappa_results and not perc_agreement_results:\n",
    "    print(\"No agreement scores could be computed for the selected participants.\")\n",
    "else:\n",
    "    for participant in participants_to_compare:\n",
    "        kappa = kappa_results.get(participant, 'N/A')\n",
    "        perc_agreement = perc_agreement_results.get(participant, 'N/A')\n",
    "        print(f\"Participant {participant}: Kappa={kappa:.4f} | Percent Agreement={perc_agreement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8f192-51ac-4bb5-a6c8-9bf7fcbc6ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
